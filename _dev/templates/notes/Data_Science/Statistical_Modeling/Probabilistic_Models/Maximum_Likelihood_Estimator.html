<p>The maximum likelihood estimator (MLE)</p>
<h2 id="coin-example">Coin Example</h2>
<p>Given a biased coin with probability <span class="math inline">\(p\)</span> to be heads and <span class="math inline">\(1-p\)</span> to be tails. Given <span class="math inline">\(n\)</span> flips we have <span class="math inline">\(X=x\)</span> heads following the binomial distribution which is going to be our likelihood function,</p>
<p><span class="math display">\[
X \sim B(n,p)
\]</span></p>
<p><span class="math display">\[
P(x \mid p) = {n \choose x} p^x (1-p)^{n-x}
\]</span></p>
<p>We want the optimal value <span class="math inline">\(p^*\)</span> that maximizes our the probability of our data <span class="math inline">\(x\)</span> (i.e., the number of heads observed). This is the definition of <strong>maximum likelihood</strong>.</p>
<p><span class="math display">\[
p^* = \mathop{\arg\max}_p{P(x \mid p)}
\]</span></p>
<h2 id="gaussian">Gaussian</h2>
<p>The parameters of the gaussian likelihood is of course its mean <span class="math inline">\(\mu_c\)</span> and variance <span class="math inline">\(\sigma_c^2\)</span> for each class <span class="math inline">\(c\)</span>.</p>
<h3 id="gaussian-mle-classifier">Gaussian MLE Classifier</h3>
<p>The solution for the QDA Gaussian classifier using MLE is,</p>
<p><span class="math display">\[
\hat\mu = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p><span class="math display">\[
\hat \sigma^2 = \frac{1}{dn} \sum_{i=1}^n |x_i - \hat\mu|^2
\]</span></p>
<p>For LDA Gaussian classifier using MLE,</p>
<p><span class="math display">\[
\hat\mu = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p><span class="math display">\[
\hat \sigma^2 = \frac{1}{dn} \sum_{c}\sum_{i : y_i = c}^n |x_i - \hat\mu_c|^2
\]</span></p>
