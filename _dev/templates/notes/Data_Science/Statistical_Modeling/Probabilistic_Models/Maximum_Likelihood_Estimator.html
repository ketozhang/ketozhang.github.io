<p>The maximum likelihood estimator (MLE)</p>
<h2 id="motivation-from-probability-theory">Motivation from Probability Theory</h2>
<p>The MLE often used without proof whether or not the underlying system is probabilistic. To understand why we are allowed to do this we need to first understanda truly probablistic system.</p>
<p>We assume each point in of our sample comes from an IID sample <span class="math inline">\(X:~X_1, X_2, \ldots, X_n\)</span>. Our goal is to determine the underlying distribution where the sample came from. For simplicity let the distribution only be dependent on one parameter <span class="math inline">\(\theta\)</span> (e.g., Bernoulli, Poisson, Exponential). The objective is stated as:</p>
<blockquote>
<p>Of all possible values of the parameter <span class="math inline">\(\theta\)</span>, which value maximizes the likelihood of obtaining the sample?</p>
</blockquote>
<p>Mathematically this statement solves the objective:</p>
<p><span class="math display">\[
\max_{\theta} \mathcal L(\theta)~, \quad\text{for}~ \mathcal L(\theta) = P( X_1, X_2, \ldots, X_n \mid \theta)
\]</span></p>
<p>Where <span class="math inline">\(\mathcal L\)</span> is called the <strong>likelihood function</strong></p>
<div class="Example">
<p>Let <span class="math inline">\(X\)</span> be a sample of <span class="math inline">\(n\)</span> IID <span class="math inline">\(\text{Normal}(\mu, \sigma^2)\)</span>. Though the normal has two parameters, we just want to find out whatâ€™s the best guess for <span class="math inline">\(\mu\)</span>.</p>
<p>The likelihood function for this sample is,</p>
<p><span class="math display">\[
\begin{align*}
\mathcal L(\mu) &amp;= \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left[-\frac{1}{2}\left(\frac{X_i-\mu}{\sigma}\right)^2\right]\\
                &amp;= (2\pi \sigma^2)^{n/2} \exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^{n} \left(X_i-\mu\right)^2\right]
\end{align*}
\]</span></p>
<p>It looks like taking the log of the likelihood would make the objective function easier to solve. Let <span class="math inline">\(L\)</span> be the log likelihood:</p>
<p><span class="math display">\[
L(\mu) = \log{\left[(2\pi \sigma^2)^{n/2}\right]} -\frac{1}{2\sigma^2}\sum_{i=1}^{n} \left(X_i-\mu\right)^2
\]</span></p>
<p>Because the objective function is only dependent of <span class="math inline">\(\mu\)</span> we can remove all constant terms:</p>
<p><span class="math display">\[
L(\mu) \propto -\frac{1}{2\sigma}\sum_{i=1}^{n} \left(X_i-\mu\right)^2
\]</span></p>
<p>Let the extremum <span class="math inline">\(\hat \mu\)</span> be the estimator for <span class="math inline">\(\mu\)</span>,</p>
<p><span class="math display">\[
\begin{gather*}
\frac{d}{d\mu} L = \frac{1}{\sigma}\sum_{i=1}^{n} \left(X_i-\mu\right) \\
\Big\Downarrow\\
\frac{1}{\sigma}\sum_{i=1}^{n} \left(X_i-\hat \mu\right) = 0\\
\Big\Downarrow\\
\boxed{\hat \mu = \frac{1}{n}\sum_{i=1}^n X_i = \bar X}
\end{gather*}
\]</span></p>
<p>We end up with the unbiased estimator for <span class="math inline">\(\mu\)</span>.</p>
<p>For <span class="math inline">\(\sigma^2\)</span>, keep <span class="math inline">\(\mu=\hat\mu\)</span> fixed and the extremum <span class="math inline">\(\hat \sigma^2\)</span> is the estimator for <span class="math inline">\(\sigma^2\)</span></p>
<p><span class="math display">\[
\boxed{\hat \sigma^2 = \frac{1}{n}\sum_{i=1}^n \left(X_i - \bar X\right)^{2}}
\]</span></p>
<p>We end up with the biased estimator for <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div class="Example">
<p>Now for a coin toss. Let <span class="math inline">\(X\)</span> be a sample of <span class="math inline">\(n\)</span> IID <span class="math inline">\(\text{Bernoulli}(p)\)</span>. The likelihood function is,</p>
<p><span class="math display">\[
\mathcal L(p) = \prod_{X_i = 1} p\prod_{X_i = 0} (1-p)
\]</span></p>
<p>This function can be simplified by noticing there if there are <span class="math inline">\(s\)</span> successes with chance <span class="math inline">\(p^s\)</span> then there are <span class="math inline">\(n-s\)</span> failures with chance <span class="math inline">\((1-p)^{n-s}\)</span>. This motivate us to define the random variable <span class="math inline">\(S = \sum_{i=1}^n X_i\)</span>.</p>
<p><span class="math display">\[
\mathcal L(p) = p^{S}(1-p)^{n-S}
\]</span></p>
<p>Noticeably, this is the binomial distribution (if <span class="math inline">\(p\)</span> was fixed) without the binomial coefficient. This is because our observed data is one sequence of <span class="math inline">\(n\)</span> Bernoulli trials. However this technical detail is not necessary because the binomial coefficient dissapears when maximizing the likelihood.</p>
<p><span class="math display">\[
L(p) = S \log p +  (n-S) \log(1-p)
\]</span></p>
<p>Finding the extremum <span class="math inline">\(\hat p\)</span>,</p>
<p><span class="math display">\[
\begin{gather*}
\frac{d}{dp}L = \frac{S}{p} - \frac{n-S}{1-p}   \\
\frac{S}{\hat p} - \frac{n-S}{1-\hat p} = 0     \\
\boxed{\hat p = \frac{S}{n} = \bar X}
\end{gather*}
\]</span></p>
</div>
