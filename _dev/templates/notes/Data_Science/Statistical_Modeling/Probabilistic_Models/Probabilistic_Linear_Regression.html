<p>Linear regression on random variables (i.e., <strong>probabilistic linear regression</strong>) builds the foundation for linear regression on empirical data. Here on, take <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> to be some random variable; for useful cases, these two random variables should not be independent.</p>
<p>The major difference between probabilistic and empirical linear regression other than nature of its variable is what we know about <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In probabilistic linear regression, one may have an insight on <span class="math inline">\(X\)</span> and/or <span class="math inline">\(Y\)</span>. From here on, take the mean, SD, and correlation of <span class="math inline">\(X, Y\)</span> be <span class="math inline">\(\mu_X,~ \mu_Y,~ \sigma_X,~ \sigma_Y,~ r(X,Y)\)</span>.</p>
<p>Because the best least squares predictor <span class="math inline">\(\hat y(X) = E(Y \mid X)\)</span> is rarely known, we’d like to model <span class="math inline">\(E(Y \mid X)\)</span>. The linear regression model is any model in the form of,</p>
<p><span class="math display">\[
\hat y(X)  = g(X) ~ \theta
\]</span></p>
<ul>
<li><p><span class="math inline">\(X\)</span>: In general a random vector prepend with the intercept term <span class="math inline">\(X_0=1\)</span>.</p>
<span class="math display">\[X = \left[1, X_1, X_2, \ldots, X_p \right]\]</span></li>
<li><span class="math inline">\(g(X)\)</span>: Some function/transformation of <span class="math inline">\(X\)</span>. This function does not have to be linear.</li>
<li><p><span class="math inline">\(\theta\)</span>: the parameter vector where <span class="math inline">\(\theta_0\)</span> is the intercept.</p>
<p><span class="math display">\[
  \theta = \left[\theta_0, \theta_1, \ldots \theta_p \right]
  \]</span></p></li>
</ul>
<h2 id="linear-least-squares">Linear Least Squares</h2>
<p>The <strong>linear least squares</strong> or <strong>ordinary least squares</strong> (<strong>OLS</strong>) takes the linear model <span class="math inline">\(\hat y(X)\)</span> as,</p>
<p><span class="math display">\[
\hat y(X) = \theta_1 X + \theta_0
\]</span></p>
<p>The goal is to find the parameter <span class="math inline">\(\theta\)</span> to minimize the mean squared error (MSE),</p>
<p><span class="math display">\[
\min_\theta \text{MSE}(\hat y) = \min_\theta E\left[ (Y - \hat y(X))^2 \right]
\]</span></p>
<p>This problem is solvable by expanding the expectation resulting in the solution,</p>
<p><span class="math display">\[
\theta_1 = \frac{\text{Cov}(X, Y)}{\text{Var}(X, Y)} = r(X, Y) \frac{\sigma_Y}{\sigma_X}
\]</span></p>
<p><span class="math display">\[
\theta_0 = E(Y) - \theta_1 E(X)
\]</span></p>
<p>Therefore the best linear model <span class="math inline">\(Y\)</span> of <span class="math inline">\(X\)</span> is,</p>
<p><span class="math display">\[
\boxed{\hat y(X) = r(X, Y) \frac{\sigma_Y}{\sigma_X} X + \mu_Y - r(X, Y) \frac{\sigma_Y}{\sigma_X} \mu_X}
\]</span></p>
<h3 id="standard-units">Standard Units</h3>
<p>It is easier to work with standard units for the calculations above. Take <span class="math inline">\(\theta^*\)</span> to be the solution for th linear model <span class="math inline">\(\hat y(X^*)\)</span> with standard random variables <span class="math inline">\(X^*, Y^*\)</span>,</p>
<p><span class="math display">\[
\hat y(X^*) = \theta X^*
\]</span></p>
<p>The best parameters are then,</p>
<p><span class="math display">\[
\theta_0 = 0
\]</span></p>
<p><span class="math display">\[
\theta_1 = r(X,Y)
\]</span></p>
<p><span class="math display">\[
\hat y(X^*) = r(X, Y) X
\]</span></p>
<p>You can check that $ _Y y(X^*) + _Y $ is indeed <span class="math inline">\(\hat y(X)\)</span>.</p>
<div class="Note">
<p>Notice that since <span class="math inline">\(\hat y(X) = r(X,Y) X\)</span>,</p>
<p><span class="math display">\[
\hat y(X) \le X
\]</span></p>
<p>This is called the <strong>regression effect</strong>.</p>
</div>
<div class="Example">
<p><strong>Regression of Bivariate Normal</strong></p>
<p>The regression of the bivariate normal exactly follows the linear model. Recall that if <span class="math inline">\(X^*\)</span> and <span class="math inline">\(Y^*\)</span> is standard bivariate normal random variables with correlation <span class="math inline">\(\rho\)</span>. Then we can write <span class="math inline">\(Y^*\)</span> as a function of two independent standard normal random variables <span class="math inline">\(X^*\)</span> and <span class="math inline">\(Z\)</span>.</p>
<p><span class="math display">\[
Y^* = \rho X^* + \sqrt{1-\rho^2}Z,
\]</span></p>
<p>We can solve for the linear least squares parameters directly or, more quickly, you can just take the conditional expectation,</p>
<p><span class="math display">\[
\begin{align*}
\hat y(X^*) &amp;= E(Y^* \mid X^*) \\
&amp;= \rho X^*
\end{align*}
\]</span></p>
<p>You may check that <span class="math inline">\(\theta_0 = 0\)</span> and <span class="math inline">\(\theta_1 = r(X, Y) = \rho\)</span>.</p>
<p>In general for all bivariate normal <span class="math inline">\(X, Y\)</span>, we take the invert the standardized transformation,</p>
<p><span class="math display">\[
\begin{align}
E(Y \mid X) &amp;= \sigma_Y E(Y^* \mid X^*) + \mu_Y \\
&amp;= \sigma_Y \hat y(X^*) + \mu_Y \\
&amp;= \sigma_Y \rho X^* + \mu_Y \\
&amp;= \frac{\sigma_Y}{\sigma_X}\rho X + \mu_Y - \frac{\sigma_Y}{\sigma_X}\rho\mu_X
\end{align}
\]</span></p>
<p>Although it’s the best predictor, that doesn’t mean it’s a good prediction. The prediction error (MSE) in standard units is given by,</p>
<p><span class="math display">\[
\text{Var}(Y^* \mid X^*) = 1-\rho^2
\]</span></p>
<p>It’s not a surprise that the prediction error is depends on the correlation. At worst case, <span class="math inline">\(\rho=0\)</span> so the error is at maximum.</p>
</div>
