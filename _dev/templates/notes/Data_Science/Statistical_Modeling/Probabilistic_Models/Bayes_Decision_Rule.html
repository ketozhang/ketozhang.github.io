<p>For some loss function <span class="math inline">\(L\)</span> the <strong>Bayes Decision Rule</strong> simply chooses the class with the maximum posterior distribution scaled by the loss of misclassification. For the cases of two classes, the decision rule looks like,</p>
<p><span class="math display">\[
\hat y(x) = \mathop{\arg\max}_A L(\neg A,A)P(A\mid x)
\]</span></p>
<p>A motivation why the sign is greater than is because we assign a greater loss if say class <span class="math inline">\(A\)</span> was classified wrong. If it was classified wrong, there should be a greater chance that the classifier will change its decision rule to choose classify <span class="math inline">\(A\)</span> better.</p>
<h2 id="zero-one-loss">Zero-One Loss</h2>
<p>The risk function for the 0-1 loss is,</p>
<p><span class="math display">\[
R(\hat y) = P(\hat y(x)\text{ is wrong})
\]</span></p>
<p>This suggest that the decision boundary is the set of all points where <span class="math inline">\(\set{x : P(A \mid x) = 0.5}\)</span></p>
<h2 id="gaussian-decision-rule">Gaussian Decision Rule</h2>
<p>For the case of two classes <span class="math inline">\(Y \in \set{1,2}\)</span> such that each class has a <span class="math inline">\(\text{Normal}(\mu_y, \sigma_y^2)\)</span> distribution and a possible prior for the chance that a sample comes from a class <span class="math inline">\(P(Y=y)\)</span> called the mixing parameter, the probability of a sample <span class="math inline">\(x\)</span> is,</p>
<p><span class="math display">\[
P(X=x) = P(X=x \mid Y=1)P(Y=1) + P(X=x \mid Y=1)P(Y=2)
\]</span></p>
<p>Thus the decision rule follows,</p>
<p><span class="math display">\[
\hat y(x) = \mathop{\arg\max}_y P(X=x\mid Y=y) P(Y=y)
\]</span></p>
