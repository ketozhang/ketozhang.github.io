<h2 id="motivating-example">Motivating Example</h2>
<p>Consider a coin where we do not know the chance of heads <span class="math inline">\(p\)</span>. Let’s take the prior that <span class="math inline">\(p\)</span> is uniformally any value between 0 and 1. That is,</p>
<p><span class="math display">\[
p \sim \text{Uniform}(0, 1)
\]</span></p>
<p>The chance that the coin lands head depends on the value of <span class="math inline">\(p\)</span> which is random. We can marginalize over all possible values of <span class="math inline">\(p\)</span>,</p>
<p><span class="math display">\[
P(H) = \int_0^1 P(H, p) ~dp = \int_0^1 P(H \mid p) f(p) ~dp
\]</span></p>
<p>In fact this looks like an expected value as a function of <span class="math inline">\(p\)</span>,</p>
<p><span class="math display">\[
P(H) = E(P(H \mid p))
\]</span></p>
<p>The chance of head given <span class="math inline">\(p\)</span> is of course <span class="math inline">\(p\)</span> itself so,</p>
<p><span class="math display">\[
\boxed{P(H) = E(p) = \frac{1}{2}}
\]</span></p>
<p>This is not so surprising since <span class="math inline">\(p\)</span> is uniform, a large sample of <span class="math inline">\(p\)</span> would neither be advantageous to getting heads or tails.</p>
<p>However, what’s the chance of getting two heads <span class="math inline">\(P(HH)\)</span> out of two tosses?</p>
<p>We know that <span class="math inline">\(P(HH \mid p) = p^2\)</span> therefore,</p>
<p><span class="math display">\[
\boxed{P(HH) = E(p^2) = \int_0^1 p^2 ~dp = \frac{1}{3}}
\]</span></p>
<p>The answer is <strong>not</strong> <span class="math inline">\(1/4\)</span> as we should expect. That must mean the two tosses of coins are not independent when <span class="math inline">\(p\)</span> is random. To see why we need to look at the conditional probability that the second coin lands head if we know the first one lands heads for a random <span class="math inline">\(p\)</span></p>
<p><span class="math display">\[
P(H_2 \mid H_1) = \frac{P(H_1, H_2)}{P(H_1)} = \frac{2}{3}
\]</span></p>
<p>Where did the <span class="math inline">\(2/3\)</span> come from? Let’s take a look at what happens to the chance of <span class="math inline">\(p\)</span> given we know the first coin lands head. We would need to apply Bayes rule,</p>
<p><span class="math display">\[
f(p \mid H_1) = \frac{P(H_1 \mid p) f(p)}{P(H_1)} \propto p
\]</span></p>
<p>Normalizing we find that,</p>
<p><span class="math display">\[
f(p \mid H_1) = 2p
\]</span></p>
<p>Thus,</p>
<!-- TODO: WHAT? -->
<p><span class="math display">\[
P(H_2 \mid H_1) = \int_0^1 P(H_2 \mid p) \cdot f(p \mid H_1) ~dp = \frac{2}{3}
\]</span></p>
