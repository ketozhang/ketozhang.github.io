<p>The posterior distribution</p>
<h2 id="motivating-example">Motivating Example</h2>
<p>Consider a coin where we do not know the chance of heads <span class="math inline">\(p\)</span>. Let’s take the prior that <span class="math inline">\(p\)</span> is uniformally any value between 0 and 1. That is,</p>
<p><span class="math display">\[
p \sim \text{Uniform}(0, 1)
\]</span></p>
<p>The chance that the coin lands head depends on the value of <span class="math inline">\(p\)</span> which is random. We can marginalize over all possible values of <span class="math inline">\(p\)</span>,</p>
<p><span class="math display">\[
P(H) = \int_0^1 P(H, p) ~dp = \int_0^1 P(H \mid p) f(p) ~dp
\]</span></p>
<p>In fact this looks like an expected value as a function of <span class="math inline">\(p\)</span>,</p>
<p><span class="math display">\[
P(H) = E(P(H \mid p))
\]</span></p>
<p>The chance of head given <span class="math inline">\(p\)</span> is of course <span class="math inline">\(p\)</span> itself so,</p>
<p><span class="math display">\[
\boxed{P(H) = E(p) = \frac{1}{2}}
\]</span></p>
<p>This is not so surprising since <span class="math inline">\(p\)</span> is uniform, a large sample of <span class="math inline">\(p\)</span> would neither be advantageous to getting heads or tails.</p>
<p>However, what’s the chance of getting two heads <span class="math inline">\(P(HH)\)</span> out of two tosses?</p>
<p>We know that <span class="math inline">\(P(HH \mid p) = p^2\)</span> therefore,</p>
<p><span class="math display">\[
\boxed{P(HH) = E(p^2) = \int_0^1 p^2 ~dp = \frac{1}{3}}
\]</span></p>
<p>The answer is <strong>not</strong> <span class="math inline">\(1/4\)</span> as we should expect. That must mean the two tosses of coins are not independent when <span class="math inline">\(p\)</span> is random. To see why we need to look at the conditional probability that the second coin lands head if we know the first one lands heads for a random <span class="math inline">\(p\)</span></p>
<p><span class="math display">\[
P(H_2 \mid H_1) = \frac{P(H_1, H_2)}{P(H_1)} = \frac{2}{3}
\]</span></p>
<p>Where did the <span class="math inline">\(2/3\)</span> come from? Let’s take a look at what happens to the chance of <span class="math inline">\(p\)</span> given we know the first coin lands head. We would need to apply Bayes rule,</p>
<p><span class="math display">\[
f(p \mid H_1) = \frac{P(H_1 \mid p) f(p)}{P(H_1)} \propto p
\]</span></p>
<p>Normalizing we find that,</p>
<p><span class="math display">\[
f(p \mid H_1) = 2p
\]</span></p>
<p>Thus,</p>
<!-- TODO: WHAT? -->
<p><span class="math display">\[
P(H_2 \mid H_1) = \int_0^1 P(H_2 \mid p) \cdot f(p \mid H_1) ~dp = \frac{2}{3}
\]</span></p>
<h2 id="beta-and-binomial-update">Beta and Binomial Update</h2>
<p>We are interested in <span class="math inline">\(n\)</span> IID Bernoulli trials where we define the sum as <span class="math inline">\(S = \sum{I_k}\)</span>.</p>
<p>The Bayesian update rule is,</p>
<p><span class="math display">\[(p \mid S=k) \sim \text{Beta}(r+k, s+n-k)\]</span></p>
<div class="Proof">
<p>Let the prior for the parameter <span class="math inline">\(p\)</span> be distributed as,</p>
<p><span class="math display">\[
\begin{gather*}
p \sim \text{Beta}(r,s)\\
f(p) = C(r,s) \cdot p^{r-1}(1-p)^{s-1}
\end{gather*}
\]</span></p>
<p>the likelihood for <span class="math inline">\(S\)</span> is then the binomial distribution,</p>
<p><span class="math display">\[
P(S=k \mid p) = {n \choose s} p^k (1-p)^{n-k}
\]</span></p>
<p>Hence, the posterior distribution is,</p>
<p><span class="math display">\[
f(p \mid S=k) = C(r+k, s+n-k) p^{r+k-1} (1-p)^{n+s-k-1}
\]</span></p>
<p>Thus, the Bayesian update adds the number of successes <span class="math inline">\(k\)</span> and the number of failures <span class="math inline">\(n-k\)</span>.</p>
<p><span class="math display">\[
\boxed{(p \mid S=k) \sim \text{Beta}(r+k, s+n-k)}
\]</span></p>
</div>
<dl>
<dt>Expectation</dt>
<dd><span class="math display">\[E(p \mid S=k) = \frac{r+k}{r+s+n}\]</span>
</dd>
<dt>MAP</dt>
<dd><span class="math display">\[\text{mode}(p \mid S=k) = \frac{r+k-1}{r+k+n-2}\]</span>
</dd>
<dt>Transition Rule</dt>
<dd><span class="math display">\[ P(S_{n+1} = k+1 \mid S_n = k) = E(p \mid S_n=k) \]</span>
</dd>
<dt>Evidence</dt>
<dd><p>The chance of <span class="math inline">\(k\)</span> heads after <span class="math inline">\(n\)</span> tosses using the beta prior is the beta-binomial distribution.</p>
<p><span class="math display">\[
P(S_n = k) = {n \choose k}\frac{C(r,s)}{C(r+k, s+n-k)}
\]</span></p>
</dd>
</dl>
