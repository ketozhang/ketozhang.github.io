<p>Decision tree on regression differs by the loss function and the last step when predicting <span class="math inline">\(\hat y\)</span> given some sample point <span class="math inline">\(z\)</span>.</p>
<p>Cross entropy no longer makes sense but we do wish to reduce the variance of error,</p>
<p><span class="math display">\[
L = \sigma^2 = \text{Var}[y-\hat y] = \text{Var}[\epsilon]
\]</span></p>
<p><span class="math display">\[
\sigma^2 = \frac{1}{n}\sum_{i \in S} \epsilon_i - \langle \epsilon \rangle
\]</span></p>
<p><span class="math display">\[
\text{gain} = \sigma^2 - \frac{n_L\sigma_L^2 + n_R\sigma_R^2}{n_L + n_R}
\]</span></p>
<p><span class="math inline">\(S\)</span> is the set of indices representing the training sample point that landed in that node. Thus the objective function is still</p>
<p><span class="math display">\[
\mathop{\arg\max}_{S_L,\, S_R}(\text{gain})
\]</span></p>
<p>Given a decision tree and sample point <span class="math inline">\(z\)</span>, a prediction is made by taking some aggregate (usually the mean) of the <span class="math inline">\(y\)</span> values in the leaf.</p>
<p><span class="math display">\[
z = \frac{1}{n}\sum_{i \in S} y_i
\]</span></p>
<h2 id="tree-depth">Tree Depth</h2>
<p>At the edge case where there are <span class="math inline">\(n\)</span> leaves where every set <span class="math inline">\(S_i, ~ \forall i\)</span> are disjoint the decision tree perfectly classifies the training set.</p>
