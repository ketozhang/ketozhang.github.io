<p>Kernels are used like mathematical magic that is able to fit complicated curve with a collection simple functions called a kernel.</p>
<p>The motivation comes from the observation that many learning algorithms:</p>
<ul>
<li>have weights that can be written as a linear combination of sample points. <span class="math display">\[
\theta =  X^\top a
\]</span></li>
<li>have the property that inner products of the design matrix <span class="math inline">\(\Phi(X)\)</span> can be written without computing <span class="math inline">\(\Phi(X)\)</span> directly.</li>
</ul>
<p>The change is that instead of solving for the parameters <span class="math inline">\(\theta\)</span>, it is now substituted with <span class="math inline">\(a\)</span> (the linear combination coefficients) which is called the <strong>dual parameters</strong>.</p>
<p>The <strong>kernel function</strong> is defined as the calculation involving the in-sample vector <span class="math inline">\(X_i\)</span> and out of sample vector <span class="math inline">\(z\)</span></p>
<p><span class="math display">\[
k(X_i, z)
\]</span></p>
<p>The kernel matrix is defined as all possible kernel functions for the feature matrix</p>
<p><span class="math display">\[
K = XX^\top; \qquad K_{ij} = k(X_i,X_j)
\]</span></p>
