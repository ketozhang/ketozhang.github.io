<p>The probability of the first success given by <span class="math inline">\(p\)</span> to be at position <span class="math inline">\(n\)</span>.</p>
<p><span class="math display">\[
\begin{gather*}
    X \sim \text{Geometric}(p)\\
    P (X = n) = q^{n-1}p,\quad \qquad p+q=1
\end{gather*}
\]</span></p>
<dl>
<dt>Expectation</dt>
<dd><p><span class="math display">\[ E[X] = \frac{1}{p} \]</span></p>
<div class="Proof">
<p>The expected value of first success after <span class="math inline">\(W\)</span> can be done quickly by noticing</p>
<ul>
<li>There must be at least one trial.</li>
<li>If the first trial succeeds (with probability <span class="math inline">\(p\)</span>), <span class="math inline">\(W=1\)</span>.</li>
<li>If the first trial fails (with probability <span class="math inline">\(q\)</span>), the remaining trials is also distributed equallty to <span class="math inline">\(W\)</span>.</li>
</ul>
<p>Let <span class="math inline">\(W = 1 + R\)</span> where <span class="math inline">\(R\)</span> is the random variable for the number of trials after the first. Let <span class="math inline">\(W&#39;\)</span> be distributed equallty to <span class="math inline">\(W\)</span>.</p>
<p><span class="math display">\[
W = 1 + R\\
\]</span></p>
<p><span class="math display">\[
\begin{align}
    \mathbb E[W] &amp;= 1 + \mathbb E[R]\\
    &amp;= 1 + q\mathbb E[W&#39;]\\
    &amp;= 1 + q\mathbb E[W]\\
\end{align}
\]</span></p>
<p><span class="math display">\[
\mathbb E[W] = \frac{1}{q}
\]</span></p>
</div>
</dd>
<dt>Variance</dt>
<dd><p><span class="math display">\[E[X^2] = \frac{q}{p^2} \]</span></p>
<div class="Proof">
<p>We can prove this using conditional variance. Notice that <span class="math inline">\(X\)</span> can be expressed as a mixture of two distributions: a constant with chance <span class="math inline">\(p\)</span>, and IID <span class="math inline">\(1 + X^*\)</span> with chance <span class="math inline">\(q\)</span></p>
<p><span class="math display">\[
\begin{gather*}
P(X=1) = p \\
P(X = 1 + X^*) = q
\end{gather*}
\]</span></p>
<p>The variance by conditioning is then,</p>
<p><span class="math display">\[
\begin{align*}
\text{Var}(X) &amp;= [\text{Var}(1) + E(1)^2]p + [\text{Var}(1+X^*) + E(1 + X^*)^2]q - E(X)^2\\
&amp;= p + \left[\text{Var(X)} + \frac{1}{p^2} + 1 \right]q - \frac{1}{p^2}\\
&amp;= \frac{1-p}{p^2}\\
&amp;= \frac{q}{p^2}
\end{align*}
\]</span></p>
</div>
</dd>
</dl>
<h2 id="coupon-collectors-problem">Coupon Collector’s Problem</h2>
<p>Let’s say there are <span class="math inline">\(n\)</span> types of coupons and you need all <span class="math inline">\(n\)</span> to win a prize. To get a coupon you must buy some product.</p>
<ol type="1">
<li><p>What is the expected number of buys to get all <span class="math inline">\(n\)</span> coupons given uniform probability of getting any coupon.</p>
<p>Let <span class="math inline">\(X_i\)</span> be the random variable of the number of buys to get a unique coupon. You need <span class="math inline">\(n\)</span> unique coupons so the number of buys to win a prize is the sum of <span class="math inline">\(X_i\)</span>.</p>
<p><span class="math display">\[
 \mathbb E[X_i] = \frac{1}{p_i} = \frac{n}{n-(i-1)}\\
 \mathbb E\left [\sum{X_i} \right] = n \sum_{k=1}^n \frac{1}{k}
 \]</span></p></li>
</ol>
