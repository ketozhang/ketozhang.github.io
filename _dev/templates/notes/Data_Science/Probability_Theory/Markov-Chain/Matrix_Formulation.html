<h2 id="transition-matrix">Transition Matrix</h2>
<p>For simplicity, consider that <span class="math inline">\(X_t:\set{1,2,3}\)</span> for all <span class="math inline">\(t \ge 0\)</span> composes a stationary Markov chain. We can repesent a transition from <span class="math inline">\(X_t\)</span> to <span class="math inline">\(X_{t+1}\)</span> as a matrix with entries as probability <span class="math inline">\(P_{ij} = P(i \to j)\)</span> of changing from state <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>,</p>
<p><span class="math display">\[
P =
\begin{bmatrix}
P_{11} &amp; P_{12} &amp; P_{12}\\
P_{21} &amp; P_{22} &amp; P_{22}\\
P_{31} &amp; P_{32} &amp; P_{32}
\end{bmatrix}
\]</span></p>
<p>The transition matrix is simple to use just by multiplying the corresponding entries of every link in the chain.</p>
<p>For <span class="math inline">\(n\)</span>-steps, we define the probability as ending up at some state <span class="math inline">\(j\)</span> given the initial state <span class="math inline">\(i\)</span>,</p>
<p><span class="math display">\[
P_{i,j}(n) = P(X_n = j \mid X_0 = i)
\]</span></p>
<p>Because <span class="math inline">\(P_{i,j}(n)\)</span> can be represented as entries <span class="math inline">\((i,j)\)</span> of a matrix, again we can create a transition matrix <span class="math inline">\(P(n)\)</span>. We can express this transition matrix as a function of the initial transition matrix <span class="math inline">\(P(1) = P\)</span>. Let’s start with <span class="math inline">\(n=2\)</span>,</p>
<p><span class="math display">\[
\begin{align}
    P_{ij}(2) &amp;= P(X_2 = j \mid X_0 = i)\\
    &amp;= \sum_{k} P(X_1 = k, X_2 = j \mid X_0 = i)\\
    &amp;= \sum_{k} P(X_1=k \mid X_0 = i)P(X_2=j \mid X_1k)\\
    &amp;= \sum_{k} P_{ik}P_{kj}
\end{align}
\]</span></p>
<p><span class="math display">\[
P_{ij}(2) = P_i^\top P_j
\]</span></p>
<p>Thus for all entries <span class="math inline">\(i,j\)</span>,</p>
<p><span class="math display">\[
P(2) = P^2
\]</span></p>
<p>You can prove by induction,</p>
<p><span class="math display">\[
P(n) = P^n
\]</span></p>
<h2 id="probability-of-state">Probability of State</h2>
<p>The probability of a state in some iteration <span class="math inline">\(t\)</span> is the state <span class="math inline">\(i_n\)</span>,</p>
<p><span class="math display">\[ \mathbb P(X_t = i) = \mathbb P_i(t) = \left[\vec P(0)  P^t\right]_{i} \]</span> <span class="math display">\[\vec P(t) = \vec P(0) P^t\]</span></p>
<ul>
<li><span class="math inline">\(\vec P(0)\)</span> : Initial state vector</li>
<li><span class="math inline">\(P\)</span> : Transition Probability matrix (constant throughout iteration)</li>
</ul>
<h2 id="first-step-equation">First Step Equation</h2>
<p>The average time <span class="math inline">\(\tau = \avg{t}\)</span> it takes for some state in all the possible states (<span class="math inline">\(i \in \Omega\)</span>) to end up in some subset of that state (<span class="math inline">\(\Omega_f \subset \Omega\)</span>) is given by the <strong>first step equation</strong>:</p>
<p><span class="math display">\[ \tau(f) = 0 \qquad f \in \Omega_f\]</span> <span class="math display">\[ \tau(i) = 1 + \sum_{j \in \Omega} P_{ij}\tau(j) \]</span></p>
<ul>
<li>The problem of average time to reach some state <span class="math inline">\(f\)</span> in Markov chain is called the <strong>Hitting Time</strong> problem.</li>
</ul>
<h2 id="probability-of-hitting-one-before-the-other">Probability of Hitting One Before the Other</h2>
<p>Let <span class="math inline">\(\Omega_A\)</span> and <span class="math inline">\(\Omega_B\)</span> be two disjoint subsets of <span class="math inline">\(\Omega\)</span>. The probability <span class="math inline">\(\alpha(i)\)</span> that starting at the initial state <span class="math inline">\(i \in \Omega\)</span>, we reach a state in <span class="math inline">\(\Omega_A\)</span> before <span class="math inline">\(\Omega_B\)</span> is:</p>
<p><span class="math display">\[
\begin{align*}
    \alpha(a) = 1 \quad &amp;a \in \Omega_A\\
    \alpha(b) = 0 \quad &amp;b \in \Omega_B\\
    \alpha(i) = \sum_{j\in\Omega} P_{ij}\alpha(j) \quad &amp;i \not\in \Omega_A \cup \Omega_B
\end{align*}
\]</span></p>
<h2 id="stationary-distribution">Stationary Distribution</h2>
<p>When running the Markov chain we can end up at some state <span class="math inline">\(i\)</span> at some iteration <span class="math inline">\(t\)</span> such that the distribution <span class="math inline">\(P_i(t)\)</span> is stationary meaning it’s invariant under the transition probability matrix <span class="math inline">\(P\)</span>:</p>
<p><span class="math display">\[
P_i(t) = P_i(t)P
\]</span></p>
<p>Formally we have the <strong>theorem:</strong></p>
<p><span class="math inline">\(P(t_0)\)</span> is invariant if and only if the distribution,</p>
<p><span class="math display">\[
\vec P(t) = \vec P(t_0) \qquad t \ge t_0\\
\]</span></p>
