<p><span class="math display">\[
P(X=k) = e^{-\lambda} \frac{\lambda^k}{k!}
\]</span></p>
<dl>
<dt>Expected Value</dt>
<dd><span class="math display">\[\mathbb E[X] = \lambda\]</span>
</dd>
<dt>Variance</dt>
<dd><span class="math display">\[\text{Var}[N] = \lambda\]</span>
</dd>
<dt>Sums</dt>
<dd><span class="math display">\[\sum{X_i} \sim \text{Poisson}\left(\sum \lambda_i\right)\]</span>
</dd>
<dt>Mode</dt>
<dd>The mode of the Poisson is the integer part of <span class="math inline">\(\mu\)</span> only if <span class="math inline">\(\mu\)</span> is not an integer,
</dd>
</dl>
<p><span class="math display">\[
\text{mode}[\text{Poisson}(\mu)] = \begin{cases}\lfloor \mu \rfloor ~\text{and}~ \lceil \mu \rceil, &amp; \mu \in \mathbb Z^+ \cup \set{0}\\ \lfloor \mu \rfloor, &amp; \text{otherwise.} \end{cases}
\]</span></p>
<h2 id="limit-of-the-binomial-distribution">Limit of the Binomial Distribution</h2>
<p>The poisson distribution can be derived by the limit of the binomial distribution. Let <span class="math inline">\(np = \lambda\)</span> where <span class="math inline">\(\lambda\)</span> is set constant. Let <span class="math inline">\(n\)</span> head to infinity and <span class="math inline">\(p\)</span> to zero.</p>
<p>Hence you may interpret the Poisson distribution as the binomial distribution of a rare event in the limit of large number of trials.</p>
<p><span class="math display">\[
\begin{gather*}
    P_k = P(X=k) = {n \choose k}p^k(1-p)^{n-k}\\
    P_0 \approx e^{-\lambda}\\
    \frac{P_k}{P_{k-1}} \approx \frac{\lambda}{k}\\
    P_k = P_0 \prod_{l=1}^k \frac{P_l}{P_{l-1}} \approx e^{-\lambda}\frac{\lambda^k}{k!}
\end{gather*}
\]</span></p>
<h2 id="relationship-with-bernoulli-and-binomial">Relationship with Bernoulli and Binomial</h2>
<p>Given many random bernoulli trials where the number of trials is distributed as <span class="math inline">\(N \sim \text{Poisson}(\lambda)\)</span> and the number of successes is <span class="math inline">\(S \sim \sum X_i\)</span> where <span class="math inline">\(X_i \sim \text{Bernoulli}(p)\)</span>, the joint distribution is captures the binomial,</p>
<p><span class="math display">\[
P(N=n, S=s) = \text{Poisson}(\lambda) \text{Binomial}(n, p)\\
\boxed{P(S=s \mid N=n) = \text{Binomial}(n, p)}
\]</span></p>
<p>The distribution of <span class="math inline">\(S\)</span> can now be determined,</p>
<p><span class="math display">\[
\begin{align}
P(S=s) &amp;= \sum_{n=0}^\infty P(N=n, S=s) \\
&amp;= \sum_{n=s}^\infty P(N=n, S=s)\\
\end{align}
\]</span></p>
<p><span class="math display">\[
\boxed{P(S=s) = \text{Poisson}(\mu p)}
\]</span></p>
<p>What’s amazing is that success <span class="math inline">\(S\)</span> and failure <span class="math inline">\(F\)</span> are independent,</p>
<p><span class="math display">\[
\boxed{P(S=s, F=f) = \text{Poisson}(\mu p)\text{Poisson}(\mu q)}
\]</span></p>
<p>This is due to the effect of randomization of a parameter (trial size) of a distribution (multiple Bernouli trials).</p>
<h2 id="relationship-with-multinomial">Relationship with Multinomial</h2>
<p>Recall the multinomial distribution as an extension of the binomial distribution.</p>
<p>To get started, let’s write a multinomial trial of three random variables <span class="math inline">\(X, Y, Z\)</span> such that representing the number of outcomes of their individual categories such that <span class="math inline">\(N=X+Y+Z\)</span>.</p>
<p><span class="math display">\[
\begin{align}
P(X,Y,Z) &amp;= {N \choose x} {N-x \choose y} (p_x)^x(p_y)^y(p_z)^z\\
&amp;= {N \choose x} {N-x \choose y} (p_x)^x(p_y)^y(p_z)^{n-x-y}
\end{align}
\]</span></p>
<p>In general we can make an even more simple expression,</p>
<p><span class="math display">\[
P(N_1, N_2, \ldots) = n!\prod_k\frac{p_k}{n_k!}
\]</span></p>
