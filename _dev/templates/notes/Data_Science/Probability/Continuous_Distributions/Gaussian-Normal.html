<p>The most prevalent distribution appearing in countless fields is the Gaussian or normal distribution.</p>
<p><span class="math display">\[X \sim N(\mu, \sigma)\]</span> <span class="math display">\[f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]</span></p>
<dl>
<dt>Standard Normal Distribution</dt>
<dd><p>The standard normal distribution is <span class="math inline">\(N(0,1)\)</span> which plays an important role in motivating why we standardize random variables. Say for a Gaussian random variable <span class="math inline">\(X\)</span>,</p>
<p><span class="math display">\[ Z = \frac{X-\mu_X}{\sigma_X} \iff Z \sim N(0,1)\]</span></p>
<p>Where <span class="math inline">\(Z\)</span> has the PDF known as the <strong>standard normal distribution</strong>,</p>
<p><span class="math display">\[
\phi(z) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2}
\]</span></p>
</dd>
<dt>Cumulative Density Function</dt>
<dd>The CDF of the normal distribution is,
</dd>
</dl>
<p><span class="math display">\[
\Phi(x) = \int_{-\infty}^{x} \phi\left(\frac{x-\mu}{\sigma}\right) dx
\]</span></p>
<dl>
<dt>Expectation</dt>
<dd><p>The expected value of the normal distribution is famously <span class="math inline">\(\mu\)</span>,</p>
<p><span class="math display">\[
\text{E}(X) = \mu
\]</span></p>
</dd>
<dt>Variance</dt>
<dd><p>The variance of the normal distribution is famously <span class="math inline">\(\sigma^2\)</span></p>
<p><span class="math display">\[
\text{Var}(X) = \sigma^2
\]</span></p>
</dd>
<dt>Sum</dt>
<dd><p>The sum of multiple normal random variables is also normal with mean and variance</p>
<p><span class="math display">\[
\begin{gather*}
\sum_k^n X_k \sim \text{Normal}(\mu, \sigma)\\
\mu = \sum_k^n{\mu_k}\\
\sigma^2 = \sum_k^n{\sigma_k^2}
\end{gather*}
\]</span></p>
</dd>
<dt>Independent Joint Probability (Rotational Invariant)</dt>
<dd><p>Due to the distributionâ€™s property of rotational invariance, the joint property of two iid Gaussian is a Gaussian however you rotate the random variable axes. Even better the Gaussian has the mean and variance of the sums of the two Gaussians.</p>
<p><span class="math display">\[
P(X,Y) = \text{N}(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)
\]</span></p>
</dd>
<dt>MGF</dt>
<dd><p>For the standard normal random variable <span class="math inline">\(Z\)</span>,</p>
<p><span class="math display">\[
M_Z(t) = e^{t^2/2}
\]</span></p>
<p>We can apply linear transformation to find the MGF for the normal distribution for the random variable <span class="math inline">\(X = \sigma Z + \mu\)</span>,</p>
<p><span class="math display">\[
M_{\sigma Z + \mu}(t) = e^{\mu t + \sigma^2 t^2/2}
\]</span></p>
<p>Notably, any distribution with an MGF that is the exponential of a degree 2 polynomial is a normal distribution</p>
</dd>
<dt>Characteristics Function</dt>
<dd><span class="math display">\[\tilde p(x) = \exp{\left[-ik\mu -\frac{k^2\sigma^2}{2}\right]} \]</span>
</dd>
<dt>Cumulants and Moments</dt>
<dd><span class="math display">\[
    \avg{x}_c = \mu ,\quad \avg{x^2}_c = \sigma^2,\quad \avg{x^n}_c = 0, \quad \ldots,\quad  \avg{x^n}_c = 0\\
    \avg{x} = \mu ,\quad \avg{x^2} = \sigma^2 + \mu^2,\quad \avg{x^n} = 3\sigma^2\mu + \mu^3, \quad \ldots
\]</span>
</dd>
</dl>
<h2 id="multivariate-gaussian">Multivariate Gaussian</h2>
<p>For a vector of random variables <span class="math inline">\(\boldsymbol x\)</span>, the multidimensional or multivariate gaussian is given by</p>
<p><span class="math display">\[
\boldsymbol X \sim \mathcal N(\boldsymbol{\mu}, \Sigma)\\
P(x) = (2 \pi \det \Sigma)^{-1/2}\exp\left[-\frac{1}{2}(\boldsymbol x- \boldsymbol \mu)^T \Sigma^{-1} (\boldsymbol x- \boldsymbol \mu)\right]
\]</span></p>
<ul>
<li><span class="math inline">\(\Sigma\)</span> : The covariance matrix</li>
</ul>
<p>More compact is to treat <span class="math inline">\((\boldsymbol x- \boldsymbol \mu)^T \Sigma^{-1} (\boldsymbol x- \boldsymbol \mu)\)</span> as the squared distance of some vector <span class="math inline">\(\Sigma^{-1/2} \boldsymbol \Delta\)</span> where <span class="math inline">\(\boldsymbol\Delta = \boldsymbol{x} - \boldsymbol{\mu}\)</span> which is known as the <strong>deviation vector</strong>. This compact form is given as,</p>
<p><span class="math display">\[
P(\boldsymbol\Delta) = (2 \pi \det \Sigma)^{-1/2}\exp\left[-\frac{1}{2}\left\lvert\Sigma^{-1/2} \boldsymbol \Delta\right\rvert^2\right]
\]</span></p>
<h3 id="covariance-matrix">Covariance Matrix</h3>
<p>The covariance matrix <span class="math inline">\(\Sigma\)</span> is a semipositive definite (symmetric) matrix</p>
<dl>
<dt>Diagonal Covariance Matrix</dt>
<dd><p>If the covariance matrix is diagonal then</p>
<p><span class="math display">\[ P(\boldsymbol{x}) = \prod{P(X_i)} \]</span></p>
</dd>
</dl>
<h2 id="relation-to-rayleigh-distribution">Relation to Rayleigh Distribution</h2>
<p>Given two independent random variable <span class="math inline">\(X,Y\)</span> with standard normal distribution, let</p>
<p><span class="math display">\[
R = \sqrt{X^2 + Y^2}
\]</span></p>
<p>Then <span class="math inline">\(R\)</span> is a the Rayleigh distribution of scale <span class="math inline">\(\sigma = 1\)</span>,</p>
<p><span class="math display">\[
f_R(r) = re^{-\frac{1}{2}r^2}, \quad \text{for } r&gt;0
\]</span></p>
<h2 id="relation-to-chi-square-distribution">Relation to Chi-Square Distribution</h2>
<p>See <a href="./Chi-Square#Relation_to_Normal_Distribution">Chi-Square</a>.</p>
