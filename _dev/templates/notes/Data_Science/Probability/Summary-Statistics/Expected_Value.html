<p>The <strong>expected value</strong> (aka <strong>expectation</strong>) is defined as,</p>
<p><span class="math display">\[
\mathbb E[X] \equiv \sum_{x \in \Omega_X}xP(x)
\]</span></p>
<h2 id="method-of-indicators">Method of Indicators</h2>
<p>If a random variable <span class="math inline">\(X\)</span> can be expressed as a sum of indicators then its expected value also follows by linearity.</p>
<p><span class="math display">\[
\mathbb E[X] = \sum_{k=1}^N \mathbb E[I_k]
\]</span></p>
<h2 id="tail-sum-formula">Tail Sum Formula</h2>
<p>The expected value can be expressed equivalently as the sum of the right tail of the distribution,</p>
<p><span class="math display">\[
\mathbb E[X] = \sum_{x \in \Omega_X} P(X &gt; x)
\]</span></p>
<h2 id="unbiased-estimator">Unbiased Estimator</h2>
<p>Consider iid random variables <span class="math inline">\(X_1, X_2, \ldots, X_n \sim \text{Uniform}(N)\)</span> where each random variable can take the values <span class="math inline">\(1,2,\ldots,N\)</span>. Notice that</p>
<p><span class="math display">\[
\mathbb E[X_k] = \sum_{i=1}^n i = \frac{N+1}{2}
\]</span></p>
<p>Let the sample mean be,</p>
<p><span class="math display">\[
\bar X \equiv \frac{1}{n}\sum_{i=1}^n X_i
\]</span></p>
<p>The expectation value of the sample mean is</p>
<p><span class="math display">\[
\mathbb E[\bar X] = \frac{N+1}{2}
\]</span></p>
<h2 id="expectation-and-conditioning">Expectation and Conditioning</h2>
<p>The expected value can be expressed in conditional probabilities. Consider the random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the expected value can be expressed using both random variable</p>
<p><span class="math display">\[
\mathbb E[X] = \sum_{x \in \Omega_X} xP(Y \mid X=x) P(X=x)\\
\mathbb E[X] = \sum_{y \in \Omega_y} xP(X \mid Y=y) P(Y=y)
\]</span></p>
<p>Furthermore, the <strong>conditional expectation</strong></p>
<p><span class="math display">\[\mathbb E[X \mid Y=y] = \sum_{x \in \Omega_X} x P(X=x \mid Y=y)\]</span></p>
<p>is a random variable because the conditional expectaiton is a function of the random variable <span class="math inline">\(Y\)</span>.</p>
<p>The conditional expectation can also have an expectation value,</p>
<p>[ <span class="math display">\[\begin{align}
    \mathbb E \Big[\mathbb E[X \mid Y] \Big] &amp;= \sum_{x \in \Omega_X}\left[\sum_{y \in \Omega_Y} y P(Y=y \mid X=x)\right]P(X=x)\\
    &amp;= \sum_{x \in \Omega_X} \mathbb E[Y \mid X=x] P(X=x)
\end{align}\]</span> ]</p>
<p>This is exactly the same as <span class="math inline">\(E[X]\)</span>.</p>
<h3 id="conditional-expectation-of-sums">Conditional Expectation of Sums</h3>
<p>Let the sum of some event be <span class="math inline">\(S = X_1 + X_2 + \ldots X_N\)</span> for iid <span class="math inline">\(X_k\)</span>. <span class="math inline">\(N\)</span> is the random variable independent from <span class="math inline">\(X\)</span> representing the count of sampling.</p>
<p>Notice that the conditional expectation is can be easily written as,</p>
<p><span class="math display">\[
\mathbb E[S \mid N = n] = n \cdot \mathbb E[S]\\
\mathbb E[S \mid N] = N \cdot \mathbb E[S]
\]</span></p>
<p>then it follows that</p>
<p><span class="math display">\[
\mathbb E[S] = \mathbb E(\mathbb E[S \mid N])\\
\boxed{\mathbb E[S] = \mathbb E[S]\mathbb E[N]}
\]</span></p>
<p>Letâ€™s call this the <strong>separation of expectation by conditioning</strong>.</p>
