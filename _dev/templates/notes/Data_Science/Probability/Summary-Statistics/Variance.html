<p>The variance is motivated by wanting to know how far away is <span class="math inline">\(E[X] = \mu\)</span> away from some true answer in the random variable <span class="math inline">\(X\)</span>. That is, the variance is the expected value of the squared deviance of <span class="math inline">\(\mu\)</span> or the <strong>root mean square</strong> of <span class="math inline">\(X\)</span>. The variance is defined as,</p>
<p><span class="math display">\[\text{Var}[X] =  E\left[ (X-\mu)^2 \right]\]</span></p>
<dl>
<dt>Non-negative</dt>
<dd><p>The variance is alway non-negative because the random variable inside is a squared variable</p>
<p><span class="math display">\[
\text{Var}[X] \ge 0
\]</span></p>
</dd>
<dt>Identity</dt>
<dd><p>A more useful identity derived by expanding the square,</p>
<p><span class="math display">\[
\text{Var}[X] =  E[X^2] -  E[X]^2
\]</span></p>
</dd>
<dt>Second Moment <span class="math inline">\(\ge\)</span> First Moment</dt>
<dd><p>In corollary, the squared expected value is always greater or equal to the expected value squared,</p>
<p><span class="math display">\[
 E[X^2] \ge  E[X]^2
\]</span></p>
</dd>
<dt>Standard Deviation</dt>
<dd><p>Because the variance is some squared quantity, we’d like to take the square root to get a sense of the untis</p>
<p><span class="math display">\[
\text{SD}[X] = \sqrt{\text{Var}[X]}
\]</span></p>
</dd>
<dt>Linearity</dt>
<dd><p>Like the expected value, the variance has its own linearity rule. The variance is invariant by translation and linear by sretching</p>
<p><span class="math display">\[
\text{Var}(aX + b) = a^2\text{Var}[X]
\]</span></p>
<p>This is the same exact property as the magnitude of a vector.</p>
</dd>
<dt>Variance of Independent Sums</dt>
<dd><p>The variance of sums is the sum of individual variance and an extra term caleld the <a href="#covariance">covariance</a></p>
<p><span class="math display">\[
\text{Var}\left[\sum_i^n X_i\right] = \sum_i^n \text{Var}[X_i] + \sum_{i \neq j} \text{Cov}[X_i,X_j]
\]</span></p>
<p>If all random variable are independent:</p>
<p><span class="math display">\[\text{Var}\left[\sum X_i\right] = \sum \text{Var}[X_i] \]</span></p>
</dd>
</dl>
<h2 id="method-of-indiciators">Method of Indiciators</h2>
<p>The variance of indicators are nice to use because,</p>
<p><span class="math display">\[
 E[I^2] =  E[I]
\]</span></p>
<p>Therefore we can easily solve for the variance of an indicator,</p>
<p><span class="math display">\[
\text{Var}[I] =  E[I] -  E[I]^2 = p(1-p)~,
\]</span></p>
<p>or simply,</p>
<p><span class="math display">\[
\boxed{\text{Var}[I] = pq}
\]</span></p>
<p>Now to use the method of indicators we actually need to rely on covariance. It’s useful to know the covariance of two indicators that are generally dependent.</p>
<p><span class="math display">\[
\begin{align*}
\text{Cov}[I_j, I_k] &amp;= E[I_jI_k] - E[I_j]E[I_k]\\
&amp;= P(I_jI_k) - P(I_j)P(I_k)\\
&amp;= p_{jk} - p_jp_k
\end{align*}
\]</span></p>
<p>By the variance of the sum of random variables,</p>
<p><span class="math display">\[
\begin{align*}
\text{Var}[I_j + I_k] &amp;= \text{Var}[I_j] + \text{Var}[I_k] + 2\text{Cov}[I_j,I_k] \\
&amp;= p_jq_j + p_kq_k + 2(p_{jk} - p_jp_k)
\end{align*}
\]</span></p>
<p>In general, for <span class="math inline">\(n\)</span> indicators,</p>
<p><span class="math display">\[
\text{Var}\left[\sum_{k=1}^n I_k \right] = \sum_{i=1}^n p_iq_i + n(n-1)\sum_{i \neq j}(p_{ij} - p_ip_j)
\]</span></p>
<h2 id="mean-squared-error">Mean Squared Error</h2>
<p>The variance is related to the mean squared error if we replace the prediciton <span class="math inline">\(\mu\)</span> with some arbritrary constant <span class="math inline">\(c\)</span>,</p>
<p><span class="math display">\[
\text{MSE}(c) =  E[(X-c)^2]
\]</span></p>
<p>By expanding $ E[(X-+-c)^2]$ out we get,</p>
<p><span class="math display">\[
\boxed{\text{MSE}(c) = \text{Var}[X] + (\mu-c)^2}
\]</span></p>
<p>Therefore any prediction <span class="math inline">\(c\)</span> is no better than <span class="math inline">\(\mu\)</span> and the MSE is at least the variance,</p>
<p><span class="math display">\[
\text{MSE}(c) \ge \sigma_X^2
\]</span></p>
<h2 id="covariance">Covariance</h2>
<p>By the motivation of summing variances, the covariance is the expected value of the product of deviances of two random variables,</p>
<p><span class="math display">\[
\text{Cov}[X,Y] = E\big[(X-\mu_X)(Y-\mu_y)\big]
\]</span></p>
<dl>
<dt>Identity</dt>
<dd><p>Expanding the covariance gives you a useful identity,</p>
<p><span class="math display">\[
\text{Cov}[X,Y] = E[XY] - \mu_X\mu_Y
\]</span></p>
</dd>
<dt>Superset of Variance</dt>
<dd><p>All variances are also covariance but with itself. This makes for a nice compact notation when needed.</p>
<p><span class="math display">\[
\text{Var}[X,X] = \text{Cov}[X,X]
\]</span></p>
</dd>
<dt>Symmetric</dt>
<dd><p>The covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is the same in reverse</p>
<p><span class="math display">\[
\text{Cov}[X,Y] = \text{Cov}[Y,X]
\]</span></p>
</dd>
<dt>Addition by Distribution</dt>
<dd><p>The addition rule for covariance is</p>
<p><span class="math display">\[
\text{Cov}[X+Y,Z] = \text{Cov}[X,Z] + \text{Cov}[Y,Z]
\]</span></p>
</dd>
<dt>Bilinearity</dt>
<dd><p>Immediately we can see constant factors are taken out just like the variance</p>
<p><span class="math display">\[
\text{Cov}[aX, bY] = ab~\text{Cov}[X,Y]
\]</span></p>
<p>In general with the additon rule, the general covariance of two summed random variable is the sum of covariances for each pairwise terms.</p>
<p><span class="math display">\[
\text{Cov}[\vec a \cdot \vec X, \vec b \cdot \vec Y] = \sum_{i,j}a_ib_j\text{Cov}[X_i, Y_j]
\]</span></p>
</dd>
<dt>Uncorrelated if Indepndent</dt>
<dd><p>If two random variables are independent then the covariance is zero,</p>
<p><span class="math display">\[
\begin{gather*}
X \perp Y \\
\big\Downarrow \\
\text{Cov}[X,Y] = 0
\end{gather*}
\]</span></p>
</dd>
<dt>Positive covariance</dt>
<dd><p>If the covariance is positive, then the two deviance term must have the same sign.</p>
<p><span class="math display">\[
\text{Cov}[X,Y] &gt; 0 \implies \begin{cases}\abs{X - E[X]} &gt; 0 &amp; \abs{Y - E[Y]} &gt; 0 \\ \abs{X - E[X]} &lt; 0 &amp; \abs{Y - E[Y]} &lt; 0 \end{cases}
\]</span></p>
</dd>
<dt>Negative covariance</dt>
<dd>If the covariance is negative,t hen the two deviance term must have opposite signs. <span class="math display">\[
\text{Cov}[X,Y] &gt; 0 \implies \Big( \abs{X - E[X]} &gt; 0 \land \abs{Y - E[Y]} &lt; 0 \Big) \lor \Big(\abs{X - E[X]} &lt; 0 \land \abs{Y - E[Y]} &gt; 0 \Big)
\]</span>
</dd>
</dl>
<h2 id="correlation">Correlation</h2>
<h3 id="properties">Properties</h3>
<ul>
<li><p><strong>Standardized Random Variables</strong> With standardized random variables <span class="math inline">\(X&#39;\)</span> and <span class="math inline">\(Y&#39;\)</span>:</p>
<p><span class="math display">\[ X&#39; = \frac{X -  E[X]}{\sigma(X)} \]</span> <span class="math display">\[ Y&#39; = \frac{Y -  E[Y]}{\sigma(Y)} \]</span></p>
<p><span class="math display">\[ \text{Corr}[X,Y] = E[X&#39;Y&#39;]  \]</span></p></li>
<li><p><strong>Unity Bound</strong></p>
<p><span class="math display">\[ -1 \le \text{Corr}[X,Y] \le 1 \]</span></p>
<dl>
<dt>Proof</dt>
<dd>Consider the standardize random variable <span class="math inline">\(X&#39;\)</span> and <span class="math inline">\(Y&#39;\)</span>. By the Cauchy-Schwarz Inequality
</dd>
</dl>
<p><span class="math display">\[ \abs{ E[X&#39;Y&#39;]} \le \abs{ E[X&#39;]  E[Y&#39;]} = 1\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[ -1 \le \text{Corr}[X,Y] \le 1  \]</span></p></li>
<li><p><strong>Perfectly Correlated or Anticorrelated</strong></p></li>
</ul>
<p><span class="math display">\[ \text{Corr}[X,Y] = 1 \implies X = Y \]</span> <span class="math display">\[ \text{Corr}[X,Y] = -1 \implies X = -Y\]</span></p>
