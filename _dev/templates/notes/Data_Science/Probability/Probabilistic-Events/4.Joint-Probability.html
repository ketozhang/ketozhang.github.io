<p>A <strong>joint probability</strong> or <strong>joint distribution</strong> is the probability of multiple random variables.</p>
<p><span class="math display">\[
    P(X_1, X_2, \ldots, X_n)
\]</span></p>
<ul>
<li>Equivalently the comma can be replaced with interesection <span class="math inline">\(\cap\)</span> or &quot;logical and&quot; <span class="math inline">\(\land\)</span></li>
</ul>
<p>In general the probability of a joint probability is given by the <strong>chain rule</strong>,</p>
<p><span class="math display">\[ \boxed{P\left(\bigcap_{i=1}^n X_i\right) = \prod_{i=1}^n P\left(X_i \Bigg\lvert \bigcap_{j\neq  i}^{n}X_j \right)}\]</span></p>
<h2 id="independent-joint-probability">Independent Joint Probability</h2>
<p>Random variables are considered independent if the their probability distribution are separable. For instance for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,</p>
<p><span class="math display">\[ P_{XY}(x,y) = P_X(x)P_Y(y) \]</span></p>
<p>A more compact notation,</p>
<p><span class="math display">\[ P(x,y) = P(x)P(y) \]</span></p>
<p>Alternatively two random variables are independent if the conditional probability is the probability of itself,</p>
<p><span class="math display">\[
    P(x \mid y) = P(x)
\]</span></p>
<h3 id="mutually-independent">Mutually Independent</h3>
<p>To generalize to a set of <span class="math inline">\(n\)</span> random variables <span class="math inline">\(\set{X_1,X_2,\ldots,X_n}\)</span>, these random variables are independent only if all subsets <span class="math inline">\(i \in I \subseteq {1,2,\ldots,n} \land \abs{I} &gt; 2\)</span> follows,</p>
<p><span class="math display">\[ \boxed{P\left(\bigcap_{i\in I} X_i\right) = \prod_{i\in I} P_{X_i}\left(x\right)}\]</span></p>
<ul>
<li><strong>NOTE</strong>: It is not enough that <span class="math inline">\(P(\cap_i^n X_i = \prod_i^n P_{X_i}(x))\)</span> to prove <span class="math inline">\(\set{X_1,X_2,\ldots,X_n}\)</span> is mutually independent.</li>
</ul>
<h2 id="marginal-probability">Marginal Probability</h2>
<p>The marginal probability is a posterior probability summing over all joint probability for each prior event. For instance given that <span class="math inline">\(X\)</span> is a random variable of the posterior and <span class="math inline">\(Y\)</span> of the prior</p>
<p><span class="math display">\[
    P_X(x) = \sum_{y \in \Omega_Y} P(x, y) = \sum_{y \in \Omega_Y}{P(x \mid y)P(y)}
\]</span></p>
<h2 id="expectation-value">Expectation Value</h2>
<p>The expectation value of a joint probability follows linearity such that,</p>
<p><span class="math display">\[ \Expected{aX+bY+c} = \sum_{x \in X}{\sum_{y \in Y}}{P(x,y)(ax+by+c)} \]</span></p>
<h2 id="union-probability">Union Probability</h2>
<h3 id="inclusion-exclusion-formula">Inclusion-Exclusion Formula</h3>
<p>The inclusion exclusion formula defines the probability of the union of two or more events.</p>
<p>Let's start simply with the union of two events which states:</p>
<center>
<strong>&quot;The probability of any event of <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> occuring.&quot;</strong>
</center>
<p><span class="math display">\[
    P(A \cup B) = P(A) + P(B) - P(A, B)
\]</span></p>
<p>In general,</p>
<p><span class="math display">\[
    P\left(\bigcup_{i=1}^n A_i\right) = \sum_i^n P(A_i) - \mathop{\sum\sum}\limits_{1\le i &lt; j \le n}P(A_iA_j) + \ldots + (-1)^{n+1}P(A_1,A_2, \ldots, A_n)
\]</span></p>
<h3 id="booles-inequality">Boole's Inequality</h3>
<p><strong>Boole's Inequality</strong> is an upper bound for the union of two or more events. It's simply says that the probability of a collection of event is no greater than the sum of all its probabilities,</p>
<p><span class="math display">\[
    P\left(\bigcup_{i=1}^nA_i\right) \le \sum_{i=1}^{n} P(A_i)
\]</span></p>
