<p><span class="math display">\[
P(X=k) = e^{-\lambda} \frac{\lambda^k}{k!}
\]</span></p>
<dl>
<dt>Expected Value</dt>
<dd><span class="math display">\[\mathbb E[X] = \lambda\]</span>
</dd>
<dt>Variance</dt>
<dd><span class="math display">\[\text{Var}[N] = \lambda\]</span>
</dd>
<dt>Sums</dt>
<dd><span class="math display">\[\sum{X_i} \sim \text{Poisson}\left(\sum \lambda_i\right)\]</span>
</dd>
<dt>Mode</dt>
<dd>The mode of the Poisson is the integer part of <span class="math inline">\(\mu\)</span> only if <span class="math inline">\(\mu\)</span> is not an integer,
</dd>
</dl>
<p><span class="math display">\[
\text{mode}[\text{Poisson}(\mu)] = \begin{cases}\lfloor \mu \rfloor ~\text{and}~ \lceil \mu \rceil, &amp; \mu \in \mathbb Z^+ \cup \set{0}\\ \lfloor \mu \rfloor, &amp; \text{otherwise.} \end{cases}
\]</span></p>
<dl>
<dt>MGF</dt>
<dd><p><span class="math display">\[
M_X(t) = \exp\left[\mu(e^{t} - 1)\right]
\]</span></p>
<div class="Proof">
<p>Directly taking the expectation and noticing that it’s the exponential taylor series,</p>
<p><span class="math display">\[
\begin{align*}
    M_X(t)  &amp;= \sum_{k=0}^\infty e^{kt} \frac{e^{-\lambda}\lambda^k}{k!}\\
            &amp;= e^{-\lambda}\sum_{k=0}^\infty \frac{e^{kt}\lambda^k}{k!} \\
            &amp;= e^{-\lambda}\sum_{k=0}^\infty \frac{(\lambda e^t)^k}{k!} \\
            &amp;= e^{-\lambda}e^{\lambda e^t} \\
    M_X(t)  &amp;= e^{\lambda (e^t - 1)}
\end{align*}
\]</span></p>
</div>
</dd>
</dl>
<h2 id="poissonization">Poissonization</h2>
<p>Poissonization is the process of letting the number of trials <span class="math inline">\(N \sim \text{Poisson}(\lambda)\)</span>. Doing so causes some families of distribution to inherit independence amongst their category. To get a better idea of what this mean let’s take a look at few distributions</p>
<dl>
<dt>Binomial</dt>
<dd>Let <span class="math inline">\(S \sim \text{Binomial}(s; N, p)\)</span>. Because <span class="math inline">\(N\)</span> is a random variable we can calculate for a all <span class="math inline">\(N\)</span> (i.e., the marginal distribution of <span class="math inline">\(S\)</span>) or the joint distribution of <span class="math inline">\(N\)</span> and <span class="math inline">\(P\)</span>,
</dd>
</dl>
<p><span class="math display">\[
  \boxed{S \sim \text{Poisson}(s; \lambda p)}
  \]</span></p>
<p><span class="math display">\[
  P(S=s, N=n): \text{Poisson}(s; \lambda p) \text{Poisson}(n-s; \lambda p)
  \]</span></p>
<p>Remarkably since <span class="math inline">\(P(S=s, N=n) = P(S=s, F=f)\)</span> for <span class="math inline">\(F\)</span> is the number of failed trials,</p>
<p><span class="math display">\[
  \boxed{P(S=s, F=f) = \text{Poisson}(s; \lambda p) \text{Poisson}(f; \lambda p) = P(S=s) P(F=f)}
  \]</span></p>
<p>This suggest <span class="math inline">\(S\)</span> and <span class="math inline">\(F\)</span> are independent!</p>
<dl>
<dt>Multinomial</dt>
<dd>The same as the binomial occurs with multionmial. For some category <span class="math inline">\(X\)</span>,
</dd>
</dl>
<p><span class="math display">\[
  P(X=x) = \text{Poisson}(s; \lambda p_x)
  \]</span></p>
<p>All categories are independent from each other and distributed as <span class="math inline">\(\text{Poisson}(x_i; \lambda_{x_i})\)</span>.</p>
<h2 id="poisson-process">Poisson Process</h2>
<p>The poisson distribution describes a process called the <strong>Poisson process</strong> which describe the chance that <span class="math inline">\(k\)</span> events occur if the rate of occurance is on average <span class="math inline">\(\mu\)</span> (a measure of number of events per unit time). However, this description does not seem complete. Let me motivate this confusion. Doesn’t it make more sense to ask what’s the chance that <span class="math inline">\(k\)</span> events occur in the next <span class="math inline">\(t\)</span> seconds. It is indeed a more natural question to ask.</p>
<h3 id="first-description---number-of-arrivals">First Description - Number of Arrivals</h3>
<p>The answer to that natural question is called the <strong>first description</strong> of the Poisson process.</p>
<p>Consider a unit interval such that:</p>
<ul>
<li>the number of events or arrivals <span class="math inline">\(N_0\)</span> in the unit interval has expectatation <span class="math inline">\(E(N_0) = \mu\)</span>.</li>
<li>The unit interval can be divided into <span class="math inline">\(n\)</span> subintervals.</li>
<li>For each subinterval the indicator of arrival is given by <span class="math inline">\(I_1, I_2, \ldots, I_n\)</span> which are Bernoulli(<span class="math inline">\(p\)</span>) trials</li>
<li><span class="math inline">\(p \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span> such that <span class="math inline">\(np \to \mu\)</span>.</li>
</ul>
<p>Given the assumption it must be that <span class="math inline">\(N_0\)</span> is distributed as <span class="math inline">\(\text{Binomial}(n, p)\)</span> with expectation <span class="math inline">\(E(N_0) = np = \mu\)</span>. As <span class="math inline">\(n \to \infty\)</span>,</p>
<p><span class="math display">\[
N_0 \sim \text{Poisson}(\mu)
\]</span></p>
<p>Now consider an interval of size <span class="math inline">\(t\)</span> (e.g., most commonly the interval <span class="math inline">\((0, t)\)</span>). It is made up of <span class="math inline">\(t\)</span> disjoint interval.</p>
<p>We make the assumption that disjoint interval are independent of each other (this is indeed the assumption made by the poisson distribution). The number of events <span class="math inline">\(N\)</span> is then the sum of events in <span class="math inline">\(t\)</span> unit intervals. Thus, <span class="math inline">\(N\)</span> is distributed as if there are <span class="math inline">\(t\)</span> IID <span class="math inline">\(\text{Poisson}(\mu)\)</span> denoted. By the sum of independent Poisson distribution,</p>
<p><span class="math display">\[
N \sim \text{Poisson}(\mu t)
\]</span></p>
<h3 id="second-description---waiting-time">Second Description - Waiting Time</h3>
<!-- TODO -->
