<p><strong>Adaptive boosting</strong> or <strong>AdaBoost</strong> is an ensemble method that:</p>
<ul>
<li>trains multiple leanrers on weighted sample points</li>
<li>use different weight for each learner</li>
<li>incraese weight of misclassified sample points</li>
<li>give bigger votes to more accurate learners</li>
</ul>
<p>Letâ€™s take the binary classification example <span class="math inline">\(y\)</span> with design matrix <span class="math inline">\(X\)</span>.</p>
<ul>
<li>Train <span class="math inline">\(T\)</span> classifiers <span class="math inline">\(G_1, \ldots, G_T\)</span></li>
<li>The weight for sample point <span class="math inline">\(X_i\)</span> in <span class="math inline">\(G_t\)</span> grows according to how <span class="math inline">\(G_1, \ldots G_{t-1}\)</span> misclassified it and shrinks on correct classification.</li>
<li>Train <span class="math inline">\(G_t\)</span> to try harder to correctly classify points with larger weights.</li>
<li><p>The metalearner is a linear combination of the ensemble.</p>
<p><span class="math display">\[
  M(z) = \sum_{t=1}^T \beta_t G_t(z)
  \]</span></p></li>
</ul>
<h2 id="algorithm">Algorithm</h2>
<p>On each iteration <span class="math inline">\(t\)</span>:</p>
<ol type="1">
<li><p>Initialize weights <span class="math inline">\(w_i = \frac{1}{n}\)</span></p></li>
<li><p>Find <span class="math inline">\(G_t\)</span> that minimizes the metalearner: <span class="math display">\[
 L(\rho, \ell) = e^{-\rho \ell} \qquad \ell \in \{-1, 1\}
 \]</span></p>
<p><span class="math display">\[
 \begin{align*}
 \mathcal L &amp;= \frac{1}{n}\sum_{n=1}^n L(M(X_i), y_i), \qquad M(X_i)=\sum_{t=1}^T \beta_t G_t (X_i)\\
 &amp;= \sum{w_i^{(T)}}e^{-\beta_Ty_iG_T(X_i)}
 \end{align*}
 \]</span></p>
<p><span class="math display">\[
 w_i^{(T)} = \prod_{t=1}^{T-1}\exp\left[-\beta_ty_iG_t(X_i)\right]
 \]</span></p>
<p><span class="math display">\[
 \mathcal L  = e^{-\beta_T}\sum_{i=1}^n w_i^{(T)} + (e^{\beta_T} - e^{-\beta_T})\underbrace{\sum_{y_i \neq G_T(X_i)}w_i^{T}}_\text{sum of misclassified weights}
 \]</span></p>
<p><span class="math display">\[
 w_i^{(T+1)} = w_i^{(T)}e^{-\beta_Ty_iG_T(X_i)}
 \]</span></p></li>
<li><p>Find <span class="math inline">\(\beta_t\)</span> that minimizes the metalearner:</p>
<p><span class="math display">\[
 \frac{\partial}{\partial \beta_T}\mathcal L = -e^{-\beta_T}\sum_{i=1}^{n}w_i^{(T)}+\left(e^{\beta_T}+e^{-\beta_T} \right)\sum_{y_i \neq G_T(X_i)}w_i^{(T)}
 \]</span></p>
<p><span class="math display">\[
 0 = -1 + (e^{2\beta_T} + 1)\text{err}_T, \qquad \text{err}_T
 = \frac{\text{weight wrong}}{\text{all weight}}
 \]</span></p>
<p><span class="math display">\[
 \beta_T = \frac{1}{2}\log\left(\frac{1-\text{err}_T}{\text{err}_T}\right)
 \]</span></p></li>
<li><p>Reweight points <span class="math inline">\(w_i = w_i e^{\beta_t y_i G_t(i)}\)</span></p></li>
</ol>
<h2 id="facts-and-heuristics">Facts and Heuristics</h2>
<ul>
<li><p>Best for short tree (depth ~3-5) ensembles.</p></li>
<li><p>Posterior probability can be approximated via:</p>
<p><span class="math display">\[
  P(Y \mid x) \approx \frac{1}{1 + e^{-2M(x)}}
  \]</span></p></li>
<li><p>If every learner has <span class="math inline">\(s \ge \mu \ge 50\)</span> accuracy score, the metalearner training accuracy will be 100%.</p></li>
</ul>
