<!doctype html>
<html lang='en'>

<head>
    <meta charset="UTF-8">
    <title>StaticPy</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap time -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link href="https://ketozhang.github.io/StaticPy/static/base.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Code+Pro|Quicksand" rel="stylesheet">
    <link href="https://ketozhang.github.io/StaticPy/static/prism.css" rel="stylesheet">
    <link rel="shortcut icon" href="https://ketozhang.github.io/StaticPy/static/favicon.ico">

    <!-- LaTeX Stylesheet -->
    
    <div class="d-none">
    $$
        \newcommand{abs}[1]{\left | #1 \right |}
        \newcommand{set}[1]{\left\{#1\right\}}
    $$
    </div>
    
</head>

<body>
    
<div class="container-fluid" id="note">
    <h1 class="title text-center pt-3">Lecture Notes19</h1>
    <hr>
    <div class="row">
        <div class="col-lg-2 d-none d-lg-block"></div>
        <div class="col-lg-7 col-md-10">
            <h1 id="lecture-notes-19">Lecture Notes 19</h1>
<p>Recall the expectation of the squared loss of some sample <span class="math inline">\((X,Y)\)</span> with some error <span class="math inline">\(Y \equiv h(x) + \varepsilon\)</span>,</p>
<p><span class="math display">\[ 
\Expected{\left( Y - f_{\hat \theta}(x) \right)^2} = \text{Observational Variance} + (\text{Bias})^2 + \text{Model Variance}
\]</span></p>
<h2 id="model-complexity">Model Complexity</h2>
<p>Model complexity can be thought of as the &quot;capcity of the model to fit the data&quot;</p>
<h2 id="least-square-linear-regression">Least Square Linear Regression</h2>
<p>The least square linear regression is a model (denoted as either <span class="math inline">\(\hat y\)</span> or <span class="math inline">\(f_\theta(x)\)</span>) with the form,</p>
<p><span class="math display">\[ \hat y = \sum_{j=1}^{d}{\theta_j\phi_j(x)}\]</span></p>
<ul>
<li><span class="math inline">\(\theta_j\)</span> : The parameter which must be linear by definiton
<ul>
<li>When we say linear we simply mean that each term must look like <span class="math inline">\(f(\theta,\phi) = \theta\phi(x)\)</span> where <span class="math inline">\(\theta\)</span> must be a constant for <span class="math inline">\(f(\theta, \phi)\)</span> to be linear.</li>
<li>To be honest, it's a convoluted way of saying the weight/parameter must be constant.</li>
</ul></li>
<li><span class="math inline">\(\phi_j(x)\)</span> : The feature function</li>
</ul>
<p>The &quot;least square&quot; part of course means we're attempting to find the parameter <span class="math inline">\(\hat \theta\)</span> that minimizes the squared loss function</p>
<p><span class="math display">\[ \hat \theta \equiv \arg\min{\frac{1}{n}\sum_i\left(y_i - \hat y(\theta, x_i)\right)^2} \]</span></p>
<ul>
<li><span class="math inline">\(\hat \theta\)</span> : Note that the optimal paramter <span class="math inline">\(\hat theta\)</span> can be a vector of length/dimension <span class="math inline">\(d\)</span>, where <span class="math inline">\(\theta = [\theta_1, \theta_2, ..., \theta_d]\)</span></li>
</ul>
<h2 id="feature-engineering">Feature Engineering</h2>
<p>The process of transforming the inputs to a model to improve prediction accuracy</p>
<p>Allows you to: * capture domain knowledge * encode non-numeric features * express non-linear relationships</p>
<p>Basic transformations:</p>
<ul>
<li>Remove uninformative data (e.g., UID)</li>
<li>Quantitative features (e.g., age)
<ul>
<li>Non-linear transformation (e.g., log)</li>
<li>Normalize/standardize (e.g., (x-mean)/stdev)</li>
</ul></li>
<li>Categorical features (e.g, state)
<ul>
<li>One-hot-Encode transformation</li>
</ul></li>
</ul>
<h3 id="one-hot-encoding">One-Hot Encoding</h3>
<p>One-hot encoding is simply transform a categorical data into a representation of states.</p>
<p>If you're not familiar with states it goes somewhat like this. Say there are 3 states labled 'A', 'B', 'C'. Each data can either be part of state 'A', 'B', and/or 'C' denoted by either <code>1</code> (yes) or <code>0</code> (no). For example, if data 1 is in state 'A' it will be:</p>
<pre><code>Data    A   B   C
   1    1   0   0</code></pre>
<h3 id="encoding-missing-values">Encoding Missing Values</h3>
<p>For either quantitative or categorical data missing values can be encoded as:</p>
<ul>
<li>infer what the missing value should be (mean, median, infer from other columns)</li>
<li>add this missing value's name to a column called &quot;missing_col_name</li>
</ul>
<p>Never delete the whole just because there exist a missing value. There might be a reason why it is missing.</p>
<h3 id="bag-of-words-encoding-encoding-text">Bag-of-Words Encoding (Encoding Text)</h3>
<p>The bag-of-words encoding literally means there is a list of words (e.g., common words in the English alphabet) that carries the count of the words in the data you're encoding.</p>
<p>Cons * Does not preserver sentence order</p>
<h3 id="n-gram-encoding">N-Gram Encoding</h3>
<p>To solve the problem of word order instead of mapping counting a single word we count a group of size <span class="math inline">\(N\)</span>-words.</p>
<p>Cons * Extremely complex</p>
<h3 id="feature-matrix">Feature Matrix</h3>
<p>All of the transformations above is to attempt to create a quantitative matrix of features called the <strong>feature matrix</strong>. Given <span class="math inline">\(n\)</span> datasets and <span class="math inline">\(d\)</span> features the feature matrix is <span class="math inline">\(\Phi\)</span> which is a <span class="math inline">\(n \times d\)</span> matrix.</p>
<p>In our model, this is quite simply to transform to a matrix if you consider two indices <span class="math inline">\(j\)</span> is the row index for which data it's referring to and <span class="math inline">\(\phi\)</span> by definition is the feature column index</p>
<p><span class="math display">\[ \hat y = \sum_{j=1}^{d}{\theta_j\phi_j(x)} = \Phi\hat\theta\]</span></p>
<p>We will not prove this (requires matrix calculus) but <span class="math inline">\(\hat \theta\)</span> that minimizes the squared loss function is given by the <strong>normal equation</strong></p>
<p><span class="math display">\[ \hat \theta = (\Phi^T\Phi)^{-1}\Phi^TY \]</span></p>
        </div>
        <div class="col-md-2 d-none d-md-block">
            <div class="sticky-top onthispage">
                <ul id="onthispage-list" class="no-list-style">On This Page</ul>
            </div>
        </div>
    </div>
</div>

    <footer class="py-2 bg-white">
    <div class="container">
        <p class="text-right m-0"> Generated by <a href="https://github.com/ketozhang/StaticPy">StaticPy</a> |
            Designed
            by <a href="https://github.com/ketozhang">Keto Z.</a>
        </p>
    </div>
</footer>

    <!-- Scripts -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>

    <script src="https://ketozhang.github.io/StaticPy/static/datetime.js"></script>
    <script src="https://ketozhang.github.io/StaticPy/static/codeHighlight.js"></script>
    <script src="https://ketozhang.github.io/StaticPy/static/prism.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.js"></script>
    <script src="https://ketozhang.github.io/StaticPy/static/onThisPage.js"></script>

    <!-- MathJax -->
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
    </script>
    

    <!-- Bootstrap JS -->
    <!-- TODO: trim JS if not needed -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>

    <!-- React -->
    <!-- TODO: do we need React? -->
    <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react-dom.min.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/react/0.13.3/JSXTransformer.js"></script>-->


    <script>
        anchors.add();
        $(document).ready(function () {
            Prism.highlightAll();
        })
    </script>
</body>

</html>