<!doctype html>
<html lang='en'>

<head>
  <meta charset="UTF-8">
  <title>
    
    Keto | Principle Component Analysis
    
  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,400;0,700;1,400;1,700&family=Poppins&display=swap"
    rel="stylesheet">
  <link href=" https://fonts.googleapis.com/css?family=Roboto|Source+Code+Pro" rel="stylesheet">
  <link href="https://ketozhang.github.io/static/prism.css" rel="stylesheet">
  <link href="https://ketozhang.github.io/static/prism-atom-dark.css" rel="stylesheet">

  <!-- CSS Stylesheet -->
  <link href="https://ketozhang.github.io/static/favicon.ico" rel="shortcut icon">
  
  <link href="/static/main.css?22ca4bfe" rel="stylesheet">
  

  <!-- LaTeX Stylesheet -->
  
  <div class="d-none">
    $$
    \newcommand{d}{\mathrm d}
    \newcommand{avg}[1]{\expval{#1}}
    \renewcommand{vec}[1]{\vectorbold{#1}}
    $$
  </div>
  
</head>

<body>
  
<div class="container-fluid" id="note">
  <!-- <nav class="navbar navbar-expand-lg">
        <button id="sidenavCollapse" class="btn btn-info" type="button">
            <i class="fas fa-align-left"></i>
            <span>Toggle Sidebar</span>
        </button>
    </nav> -->
  <article class="row">
    <!-- Notebook Sidenav -->
    <div class="col-3 bg-light pt-1">
      <div class="sidenav sticky-top vh-80">
        <div class="sidenav-header row no-gutters">
          <div class="col-9">
            <strong>Principle Component Analysis</strong>
          </div>
          <div class="col-2 offset-1">
            <a class="go-up" href="https://ketozhang.github.io/notes/Data_Science/Statistical_Modeling/Dimensionality_Reduction/" role="button">
              <img src="https://img.icons8.com/dotty/80/000000/up.png" style="width: 25px; height: 25px" />
            </a>
          </div>
        </div>
        <div class="sidenav-body no-gutters">
          <ul class="no-list-style pl-3">
            
          </ul>
        </div>
      </div>
    </div>

    <!-- Content -->
    <div class="col-7 col-md-6">
      <h1 class="title text-center pt-3">Principle Component Analysis</h1>
      
      <div class="markdown-content">
        <p><strong>Principle Component Analysis (PCA)</strong> reduces the dimension by projecting onto another subspace that captures as much variation of the data as possible.</p>
<h2 id="gaussian-components">Gaussian Components</h2>
<p>Given cenetered data <span class="math inline">\(X\)</span>, the covariance matrix is given by</p>
<p><span class="math display">\[
\hat \Sigma = \frac{1}{n}X^\top X
\]</span></p>
<p>Project <span class="math inline">\(X\)</span> onto the principle coordinates of dimension <span class="math inline">\(k\)</span> which are the eigenvectors <span class="math inline">\(v_1 \ldots v_k\)</span> associated with a set of the largest eigenvalues <span class="math inline">\(\lambda_1 \ldots \lambda_k\)</span></p>
<h2 id="variability-accounted">Variability Accounted</h2>
<p>The percentage of variability accounted for is given by the proportion of the sum of the eigenvalues accounted for.</p>
<p><span class="math display">\[
\frac{\sum_i^k \lambda_i}{\sum_i^d \lambda_i}
\]</span></p>
<h2 id="general-pca">General PCA</h2>
<p>In general, a gaussian fit to the data is not needed. The goal of PCA once again is to find the direction of projection that maximizes variance. This can be written as a maximization problem:</p>
<p><span class="math display">\[
\begin{align}
\text{Var}(X_1&#39;, \ldots X_n&#39;) &amp;= \frac{1}{n}\sum_{i=1}^n \left(X_i \frac{w}{|w|}\right)^2\\
&amp;= \underbrace{\frac{1}{n}\frac{w^\top X^\top X w}{w^\top w}}_\text{Rayleigh Quotient}
\end{align}
\]</span></p>
<p>The identity is the Raleigh quotient of <span class="math inline">\(X^\top X\)</span> and <span class="math inline">\(w\)</span>. Therefore if <span class="math inline">\(w\)</span> is an eigenvector <span class="math inline">\(v_i\)</span> then the Rayleigh quotient is <span class="math inline">\(\lambda_i\)</span>.</p>
<p>Restricting <span class="math inline">\(w\)</span> to be an eigenvector <span class="math inline">\(v_d\)</span> maximizes the variance to <span class="math inline">\(\lambda_d / n\)</span></p>
<p>Not restricting <span class="math inline">\(w\)</span> to be an eigenvector, because <span class="math inline">\(w\)</span> is a linear combination of the eigenvectors <span class="math inline">\(v\)</span>, the Raleyigh quotient is a linear combination of the eigenvalues.</p>
<h2 id="deriving-pca-using-projection-error">Deriving PCA Using Projection Error</h2>
<p>The sum of squared error is the projection error of PCA. We want to minimize the projection error.</p>
<p><span class="math display">\[
\min \sum_{i=1}^n \left | X_i - X_i&#39; \right |^2 = \sum|X_i|^2 - \underbrace{\sum\left(X_i \cdot \frac{w}{|w|}\right)^2}_\text{Variance}
\]</span></p>
<h2 id="eigenfaces">Eigenfaces</h2>
<p>Given <span class="math inline">\(X\)</span> containing <span class="math inline">\(n\)</span> faces of <span class="math inline">\(d\)</span> grayscale pixels each, find the nearest neighbor to the training set. This is a slow process of <span class="math inline">\(\Theta(nd)\)</span>. PCA can reduce this dimension by projection <span class="math inline">\(d\)</span> to <span class="math inline">\(d&#39;\)</span>.</p>
<h2 id="random-projections">Random Projections</h2>
<p>An alternative to PCA as a preprocess. Choosing a small <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\delta\)</span> create a random subspace <span class="math inline">\(S \in \mathbb R^d\)</span> of dimension <span class="math inline">\(k\)</span> where</p>
<p><span class="math display">\[
k = \frac{2 \log \delta}{\epsilon^3 / 3 - \epsilon^2 / 2}
\]</span></p>
<p>For any point <span class="math inline">\(q\)</span>, let <span class="math inline">\(\hat q\)</span> be the orthogonal projection of <span class="math inline">\(q\)</span> onto <span class="math inline">\(S\)</span> multiplied by <span class="math inline">\(\sqrt{d/k}\)</span>.</p>
<p>For any two points <span class="math inline">\(q\)</span>, <span class="math inline">\(w \in \mathbb R^{d}\)</span></p>
<p><span class="math display">\[
(1-\epsilon)|qw|^2 \le |\overline{\hat q \hat w}|^2 \le (1+\epsilon)|\overline{qw}|^2
\]</span></p>
<p>with probability <span class="math inline">\(\mathbb P \ge 1 - 2\delta\)</span></p>
<p>The typical hyperparameters are:</p>
<p><span class="math display">\[
\varepsilon \in [0.02, 0.5]\\
\delta \in [1/n^3, 0.05]
\]</span></p>

      </div>
      
    </div>
    <!-- On This Page -->
    <div class="col-md-3 d-none d-md-block">
      <div class="onthispage sidenav small">
    <div class="sidenav-header">
        <strong>&#9776; On This Page</strong>
    </div>
    <div class="onthispage-collapse">
        <div class="sidenav-body bg-light">
            <ul class="onthispage-list no-list-style"></ul>
        </div>
    </div>
</div>
    </div>
  </article>
</div>


  <!-- Scripts -->
  <script src="https://code.jquery.com/jquery-3.4.1.min.js"
    integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

  <script src="https://ketozhang.github.io/static/js/datetime.js"></script>
  <script src="https://ketozhang.github.io/static/js/disqus.js"></script>
  <script src="https://ketozhang.github.io/static/js/codeHighlight.js"></script>
  <script src="https://ketozhang.github.io/static/js/prism.js"></script>
  <script src="https://ketozhang.github.io/static/js/onthispage.js"></script>
  <script src="https://ketozhang.github.io/static/js/markdown.js"></script>

  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://ketozhang.github.io/static/js/mathjax_config.js" defer></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>

  <!-- Bootstrap JS -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
    crossorigin="anonymous"></script>


  <!-- Custom Global scripts -->
  <script>
    $(document).ready(function () {
      Prism.highlightAll();
    })
  </script>

  <!-- Local scripts -->
  
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.js"></script>
<script>
  anchors.add();
</script>

</body>

</html>