<p>The binomial distribution is given as,</p>
<p><span class="math display">\[ P(X=k; p,n) = {n \choose k} p^k(1-p)^k \]</span></p>
<dl>
<dt>Expected Value</dt>
<dd><span class="math display">\[ \mathbb E[X] = np \]</span>
</dd>
<dt>Variance</dt>
<dd><span class="math display">\[
\text{Var}(X) = npq
\]</span>
</dd>
<dt>Moment Generating Function</dt>
<dd>The product of <span class="math inline">\(n\)</span> Bernoulli MGF, <span class="math display">\[
M_X(t) = (q+pe^t)^n
\]</span>
</dd>
</dl>
<h2 id="relations-to-bernoulli-trials">Relations to Bernoulli Trials</h2>
<p>The bernoulli distribution is the indicator decomposition of the Binomial distribution by letting <span class="math inline">\(X\)</span> be the binomial random variable that is the number of times the bernoulli trial succeeds:</p>
<p><span class="math display">\[
I_k \sim \text{Bernoulli}(p)\\
X = \sum_{k=1}^n I_k
\]</span></p>
<h2 id="poisson-limit">Poisson Limit</h2>
<p>The poisson distribution can be derived by the limit of the binomial distribution as <span class="math inline">\(np\)</span> tends to some constant <span class="math inline">\(\lambda\)</span>. Let <span class="math inline">\(np \rightarrow \lambda\)</span> where <span class="math inline">\(n \rightarrow \infty\)</span> and <span class="math inline">\(p \rightarrow 0\)</span>.</p>
<p><span class="math display">\[
\begin{gather*}
    p_k = P(X=k) = {n \choose k}p^k(1-p)^{n-k}\\
    p_0 \approx e^{-\lambda}\\
    \frac{p_k}{p_{k-1}} \approx \frac{\lambda}{k}\\
    p_k = p_0 \prod_{l=1}^k \frac{P_l}{P_{l-1}} \approx e^{-\lambda}\frac{\lambda^k}{k!}
\end{gather*}
\]</span></p>
<p><span class="math display">\[
X \sim \text{Poisson}(k; np)
\]</span></p>
<p>Hence you may interpret the Poisson distribution as the binomial distribution of a rare event in the limit of large number of trials.</p>
<p>Now there’s another limit where <span class="math inline">\(n \rightarrow \infty\)</span> and <span class="math inline">\(p \rightarrow 1\)</span> such that <span class="math inline">\(n(1-p) \rightarrow n-\lambda\)</span>. In this case, the proof is easier by solving “<span class="math inline">\(q\)</span> failures from <span class="math inline">\(n\)</span> trials”,</p>
<p><span class="math display">\[
q_{l} := p_{k} \quad k + l = n\\
\]</span></p>
<p><span class="math display">\[
\begin{align*}
    q_l &amp;= {n \choose l}q^l(1-q)^{n-l}\\
    &amp;\approx e^{-(n-\lambda)}\frac{(n-\lambda)^l}{l!}
\end{align*}
\]</span></p>
<p><span class="math display">\[
p_k = e^{-(n-\lambda)}\frac{(n-\lambda)^{(n-k)}}{(n-k)!}
\]</span></p>
<p><span class="math display">\[
\boxed{P(X=k) \approx \text{Poisson}(n-k;nq)}
\]</span></p>
<h2 id="normal-limit-of-the-binomial-distribution">Normal Limit of the Binomial Distribution</h2>
<p>Because the binomial distribution is <span class="math inline">\(n\)</span> IID Bernoulli trials of chance <span class="math inline">\(p\)</span>, at large number of trials the central limit theorem applies such that distribution converges,</p>
<p><span class="math display">\[
X \to \text{Normal}(np,~ npq)
\]</span></p>
