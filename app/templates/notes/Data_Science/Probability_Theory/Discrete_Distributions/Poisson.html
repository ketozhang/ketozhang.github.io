<p>The Poisson distribution describes the process where <span class="math inline">\(k\)</span> events occur at a unit interval given that the average number of events per unit interval is <span class="math inline">\(\lambda\)</span>.</p>
<p><span class="math display">\[
P(X=k) = e^{-\lambda} \frac{\lambda^k}{k!}
\]</span></p>
<dl>
<dt>Expected Value</dt>
<dd><span class="math display">\[\mathbb E[X] = \lambda\]</span>
</dd>
<dt>Variance</dt>
<dd><span class="math display">\[\text{Var}[X] = \lambda\]</span>
</dd>
<dt>Sums of IID Poisson</dt>
<dd><span class="math display">\[\sum{X_i} \sim \text{Poisson}\left(\textstyle\sum \lambda_i\right)\]</span>
</dd>
<dt>Mode</dt>
<dd>The mode of the Poisson is the integer part of <span class="math inline">\(\lambda\)</span> only if <span class="math inline">\(\lambda\)</span> is not an integer,
</dd>
</dl>
<p><span class="math display">\[
\text{mode}[\text{Poisson}(\lambda)] = \begin{cases}\lfloor \lambda \rfloor ~\text{and}~ \lceil \lambda \rceil, &amp; \lambda \in \mathbb Z^+ \cup \set{0}\\ \lfloor \lambda \rfloor, &amp; \text{otherwise.} \end{cases}
\]</span></p>
<dl>
<dt>MGF</dt>
<dd><p><span class="math display">\[
M_X(t) = \exp\left[\lambda(e^{t} - 1)\right]
\]</span></p>
<div class="Proof">
<p>Directly taking the expectation and noticing that it’s the exponential taylor series,</p>
<p><span class="math display">\[
\begin{align*}
    M_X(t)  &amp;= \sum_{k=0}^\infty e^{kt} \frac{e^{-\lambda}\lambda^k}{k!}\\
            &amp;= e^{-\lambda}\sum_{k=0}^\infty \frac{e^{kt}\lambda^k}{k!} \\
            &amp;= e^{-\lambda}\sum_{k=0}^\infty \frac{(\lambda e^t)^k}{k!} \\
            &amp;= e^{-\lambda}e^{\lambda e^t} \\
    M_X(t)  &amp;= e^{\lambda (e^t - 1)}
\end{align*}
\]</span></p>
</div>
</dd>
</dl>
<h2 id="poissonization">Poissonization</h2>
<p>Poissonization is the process of letting the number of trials <span class="math inline">\(N \sim \text{Poisson}(\lambda)\)</span>. Doing so causes some families of distribution to inherit independence amongst their category. To get a better idea of what this mean let’s take a look at few distributions</p>
<h2 id="binomial">Binomial</h2>
<p>Let <span class="math inline">\(S \sim \text{Binomial}(N, p)\)</span>. The marginal distribution on <span class="math inline">\(N\)</span> shows that</p>
<p><span class="math display">\[
  \boxed{S \sim \text{Binomial}(N, p) = \text{Poisson}(\lambda p)}
  \]</span></p>
<dl>
<dt>Independence of Success and Failures</dt>
<dd><p>If the number of success is <span class="math inline">\(S=s\)</span> and number of failures is <span class="math inline">\(F=f\)</span> then <span class="math inline">\(N = s + f\)</span> then,</p>
<p><span class="math display">\[
\begin{gather*}
P(S=s, F=f) = P(S=s)P(F=f)\\
S \sim \text{Poisson}(\lambda p) \quad F \sim \text{Poisson}(\lambda q)
\end{gather*}
\]</span></p>
<div class="Proof">
<p><span class="math display">\[
\begin{align*}
P(S=s, F=f) &amp;= P(N=s+f, S=s) \\
&amp;= P(N=s+f) P(S=s \mid N=s+f) \\
&amp;=e^{-\lambda}\frac{\lambda^{s+f}}{(s+f)!} \frac{(s+f)!}{s!f!}p^sq^f\\
&amp;= \left[e^{-\lambda p} \frac{(\lambda p)^s}{s!}\right] \left[e^{-\lambda q} \frac{(\lambda q)^f}{f!}\right]
\end{align*}
\]</span></p>
</div>
</dd>
</dl>
<p>This suggest <span class="math inline">\(S\)</span> and <span class="math inline">\(F\)</span> are independent!</p>
<h2 id="multinomial">Multinomial</h2>
<p>The same as the binomial occurs with multionmial. For some category <span class="math inline">\(X\)</span>,</p>
<p><span class="math display">\[
  P(X=x) = \text{Poisson}(s; \lambda p_x)
  \]</span></p>
<p>All categories are independent from each other and distributed as <span class="math inline">\(\text{Poisson}(x_i; \lambda_{x_i})\)</span>.</p>
<h2 id="poisson-process">Poisson Process</h2>
<p>The poisson distribution describes a process called the <strong>Poisson process</strong> which describe the chance that <span class="math inline">\(k\)</span> events occur if the rate of occurance is on average <span class="math inline">\(\lambda\)</span> (a measure of number of events per unit time). However, this description does not seem complete. Let me motivate this confusion. Doesn’t it make more sense to ask what’s the chance that <span class="math inline">\(k\)</span> events occur in the next <span class="math inline">\(t\)</span> seconds. It is indeed a more natural question to ask.</p>
<h3 id="first-description---number-of-arrivals">First Description - Number of Arrivals</h3>
<p>The answer to that natural question is called the <strong>first description</strong> of the Poisson process.</p>
<p>Consider a unit interval such that:</p>
<ul>
<li>the number of events or arrivals <span class="math inline">\(N_0\)</span> in the unit interval has expectation <span class="math inline">\(E(N_0) = \lambda\)</span>.</li>
<li>The unit interval can be divided into <span class="math inline">\(n\)</span> subintervals.</li>
<li>For each subinterval the indicator of arrival is given by <span class="math inline">\(I_1, I_2, \ldots, I_n\)</span> which are Bernoulli(<span class="math inline">\(p\)</span>) trials</li>
<li><span class="math inline">\(p \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span> such that <span class="math inline">\(np \to \lambda\)</span>.</li>
</ul>
<p>Given the assumption it must be that <span class="math inline">\(N_0\)</span> is distributed as <span class="math inline">\(\text{Binomial}(n, p)\)</span> with expectation <span class="math inline">\(E(N_0) = np = \lambda\)</span>. As <span class="math inline">\(n \to \infty\)</span>, the Poisson limit of the Binomial applies:</p>
<p><span class="math display">\[
N_0 \sim \text{Poisson}(\lambda)
\]</span></p>
<p>Now consider an interval of size <span class="math inline">\(t\)</span> (e.g., most commonly the interval <span class="math inline">\((0, t)\)</span>). It is made up of <span class="math inline">\(t\)</span> disjoint unit interval.</p>
<p>We make the assumption that disjoint interval are independent of each other (this is indeed the assumption made by the poisson distribution). The number of events <span class="math inline">\(N\)</span> is then the sum of events in <span class="math inline">\(t\)</span> unit intervals. Thus, <span class="math inline">\(N\)</span> is distributed as if there are <span class="math inline">\(t\)</span> IID <span class="math inline">\(\text{Poisson}(\lambda)\)</span> denoted. By the sum of independent Poisson distribution,</p>
<p><span class="math display">\[
N \sim \text{Poisson}(\lambda t)
\]</span></p>
<h3 id="second-description---inter-arrival-waiting-time">Second Description - Inter-Arrival Waiting Time</h3>
<p>Let <span class="math inline">\(G_k\)</span> be the inter-arrival time that is the length of the time interval between <span class="math inline">\(T_k\)</span> and <span class="math inline">\(T_{k-1}\)</span>, where <span class="math inline">\(T_i\)</span> is the time of the <span class="math inline">\(i\)</span>th arrival</p>
<p><span class="math display">\[
G_k = T_k - T_{k-1}
\]</span></p>
<dl>
<dt>Exponential</dt>
<dd><p><span class="math inline">\(G_k\)</span> is has the exponential distribution since,</p>
<p><span class="math display">\[
P(G_1 &gt; t) = P(N_{(0, t)} = 0) = e^{\lambda t}
\]</span></p>
<p>This is the survival function of the exponential distribution. The same can be shown for all <span class="math inline">\(k\)</span> since <span class="math inline">\(N_{(T_{k-1}, T_{k})}\)</span> are disjoint.</p>
</dd>
<dt>Memoryless property</dt>
<dd>Because each time interval is disjoint <span class="math inline">\(G_k\)</span> is mutually independent of each other <span class="math inline">\(k\)</span> for all <span class="math inline">\(k\)</span>.
</dd>
</dl>
