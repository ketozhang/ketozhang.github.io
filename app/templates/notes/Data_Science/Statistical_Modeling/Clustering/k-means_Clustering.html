<p><strong>k-means clustering</strong> or <strong>Lloyd’s Algorithm</strong> partitions <span class="math inline">\(n\)</span> points into <span class="math inline">\(k\)</span> disjoint clusters. Assign each input point <span class="math inline">\(X_i\)</span> a cluster label <span class="math inline">\(y_i \in [1,k]\)</span>.</p>
<p>We assign each of the <span class="math inline">\(k\)</span> cluster with its mean <span class="math inline">\(\mu_i\)</span>. The objective is to minimize each sample’s Euclidean distance to the cluster mean.</p>
<p><span class="math display">\[
y =  \mathop{\arg\min}_y \sum_{i=1}^k\sum_{y_j = i}|X_j - \mu_i|^2
\]</span></p>
<p>The solution is unfortunatley NP-hard in <span class="math inline">\(\mathcal O(nk^n)\)</span>.</p>
<h2 id="heuristic-solution">Heuristic Solution</h2>
<p>Because the solution is NP-hard, we use the following algorithm:</p>
<ol type="1">
<li>Let <span class="math inline">\(y_j\)</span> be fixed and update <span class="math inline">\(\mu_i\)</span>. Solved via letting <span class="math inline">\(\mu_i\)</span> be the mean of the points in cluster <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(\mu_j\)</span> are fixed and update <span class="math inline">\(y_j\)</span>. Solved via letting <span class="math inline">\(y\)</span> be the points where <span class="math inline">\(X_j\)</span> is closest to the center <span class="math inline">\(\mu_j\)</span>.</li>
<li>Repeat 1-2 until no changes.</li>
</ol>
<p>This algorithm will always terminate but the solution is not always the global minimum.</p>
<p>This algorithm is susceptible to initial clustering. There are a few methods that works:</p>
<ul>
<li><strong>Forgy method</strong>: Choose <span class="math inline">\(k\)</span> ranodm sample points to be initial the set of <span class="math inline">\(\mu\)</span> then go to step 2.</li>
<li><strong>Random partition</strong>: Randomly assign each sample point to cluster then go to step 1.</li>
<li><strong>k-means ++</strong>: Like Forgy, but biased distribution</li>
</ul>
<h2 id="alternative-objective-function">Alternative Objective Function</h2>
<p><span class="math display">\[
y = \mathop{\arg\min}_y \sum_{i=1}^k \frac{1}{n_i}\sum_{y_j=i}\sum_{y_m=i} |X_j - X_m|^2
\]</span></p>
<h2 id="k-medroids">k-Medroids</h2>
<p><strong>k-Medroids</strong> does not use Euclidean distance but some general distance function <span class="math inline">\(d(X_i, X_j)\)</span>.</p>
<p>The mean is replace with the <strong>medoid</strong> <span class="math inline">\(\mu\)</span> which is the sample point that minimizes the total distance to other points in the same cluster.</p>
