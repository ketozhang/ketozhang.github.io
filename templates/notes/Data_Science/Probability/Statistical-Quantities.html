<h2 id="expected-value">Expected Value</h2>
<p>The expected value or expectation value of <span class="math inline">\(X\)</span> is the mean of <span class="math inline">\(X\)</span>. For the discrete case, the epected value is given by,</p>
<p><span class="math display">\[\boxed{ \Expected{X} = \sum_{x}{x P(x)}}\]</span></p>
<p>The expected value has the following properties:</p>
<ul>
<li>Expected value of a constant is constant:</li>
</ul>
<p><span class="math display">\[\Expected{c} = c\]</span></p>
<ul>
<li>The previous property is all you need to understand the following linearity property. For the following take lower case letter to be contant and upper case letter to be random variables.</li>
</ul>
<p><span class="math display">\[ \Expected{aX+Y+b} = a\Expected{X} + \Expected{Y} + b \]</span></p>
<h2 id="variance">Variance</h2>
<p>The variance is defined as the average of the data's squared deviation from the mean, here deviation means <span class="math inline">\(\Delta X = X - \Expected{X}\)</span></p>
<p><span class="math display">\[
\Var{X} \equiv \Expected{(\Delta X)^2}\\
\boxed{\Var{X} \equiv \Expected{(X - \Expected{X}^2)^2}}
\]</span></p>
<p>However a more useful identity is the squared term identity,</p>
<p><span class="math display">\[
\boxed{\Var{X} = \Expected{X^2} - \Expected{X}^2}
\]</span></p>
<p>Here are some more properties:</p>
<ul>
<li>Variance of a constant is zero <span class="math display">\[\Var{a} = 0\]</span></li>
<li><p>Variance of a constant factor squares the constant factor <span class="math display">\[\Var{aX} = a^2\Var{X}\]</span></p></li>
<li><p>Linearity if Independent <span class="math display">\[ \Var{X+Y} = \Var{X} + \Var{Y} \]</span></p></li>
</ul>
<p>Another useful quantity that is used a lot more than the variance is called the <strong>standard deviation</strong>, which is just the square root of variance:</p>
<p><span class="math display">\[ \mathbf{SD}[X] = \sqrt{\Var{X}} \]</span></p>
<h2 id="covariance">Covariance</h2>
<p>The covariance describe how two variables vary jointly,</p>
<p><span class="math display">\[
 \Cov{X,Y} = \Expected{\Delta X \Delta Y} \\
\boxed{\Cov{X,Y} \equiv \Expected{(X-\Expected{X})(Y-\Expected{Y})} }
\]</span></p>
<p>A useful identity is, <span class="math display">\[
\Cov{X,y} = \Expected{XY} - \Expected{X}\Expected{Y}
\]</span></p>
<p>Some properties:</p>
<ul>
<li>By definition covariance of two independent sample is zero</li>
</ul>
<p><span class="math display">\[ \Cov{X,Y} = 0 \tag{X independent of Y} \]</span></p>
<ul>
<li><p>Covariance of constants are zero</p>
<p><span class="math display">\[ \Cov{a, b} = 0 \]</span></p></li>
<li><p>Covariance of a constant factorc can be factored out,</p>
<p><span class="math display">\[ \Cov{aX,bY} = ab\Cov{X,Y} \]</span></p></li>
</ul>
<p>Like variance, there is another version of covariance that is used more called <strong>correlation</strong>,</p>
<p><span class="math display">\[\boxed{ \Corr{X,Y} = \frac{\Cov{X,Y}}{\SD{X}\SD{Y}} }\]</span></p>
<h2 id="two-value-example">Two Value Example</h2>
<p>Let's do an example of a system of two values with probability <span class="math inline">\(p\)</span> and <span class="math inline">\(1-p\)</span>,</p>
<p><span class="math display">\[ \Expected{X} = p \\
\Var{X} = p(1-p)
\]</span></p>
<h2 id="statistics-with-multiple-samples">Statistics with Multiple Samples</h2>
<p>This part will be a bit confusing, consider that we take multiple samples labeled <span class="math inline">\(X_i\)</span> indexed with <span class="math inline">\(i\)</span>'s. A useful quantity to get from these samples is the average value (<strong>not expected value</strong>) which is also called the <strong>sample mean</strong>,</p>
<p><span class="math display">\[
\boxed{\bar{X} \equiv \frac{1}{n}\sum_{i=1}^{n}{X_i}}
\]</span></p>
<p>For the unbiased and independent samples, we may get statistic quantities from the sample mean,</p>
<p><span class="math display">\[
\Expected{\bar X} = \mu\\
\Var{\bar X} = \frac{\sigma^2}{n}\\
\SD{\bar X} = \frac{\sigma^2}{\sqrt{n}}
\]</span></p>
<ul>
<li><span class="math inline">\(\mu\)</span> : Population mean or the mean of all samples</li>
<li><span class="math inline">\(\sigma\)</span> : Standard deviation of all samples</li>
<li>$  $ : This is also known as the <strong>standard error</strong>, <span class="math inline">\(\SD{X} = \mathbf{SE}(X)\)</span></li>
</ul>
<h2 id="bootstrap-sampling">Bootstrap Sampling</h2>
<p>A bootstrap sample is a method that samples the sample.</p>
<h2 id="average-sample-loss">Average Sample Loss</h2>
<p>We will now redefine our loss function to be even more general</p>
<p><span class="math display">\[ \bar{L}(\theta) = \frac{1}{n}\sum_{i=1}^n{\ell(Y_i,f_\theta(X_i))}\]</span></p>
<ul>
<li><span class="math inline">\(\ell(Y_i,f(X_i))\)</span> : Loss function given some parametric model <span class="math inline">\(f\)</span></li>
</ul>
<h2 id="risk-and-expected-loss">Risk and Expected Loss</h2>
<p>Risk is the expected value of the loss function,</p>
<p><span class="math display">\[ R(\theta) =  \Expected{\ell(Y,f_\theta(X))}\]</span></p>
