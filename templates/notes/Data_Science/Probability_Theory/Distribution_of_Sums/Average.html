<p>The average of random variables <span class="math inline">\(X = [X_1, X_2, \ldots, X_n]\)</span> is a type of sums notably,</p>
<p><span class="math display">\[
\bar X \equiv \frac{1}{n}S =  \frac{1}{n} \sum_{k=1}^n X_k
\]</span></p>
<ul>
<li>This is very different from the sum is that <span class="math inline">\(\bar X\)</span> is continuous thus more useful in some contexts.</li>
<li>Strictly textbook, this is identical to the <strong>sample average</strong> which is only correct in the context of samples (we will get to samples later). Since all <span class="math inline">\(X_k\)</span> are random variables, <span class="math inline">\(\bar X\)</span> is also a random variable hence should never be called sample average when dealing with random variables.</li>
</ul>
<dl>
<dt>Expectation</dt>
<dd><p>Using the expectation of sums,</p>
<p><span class="math display">\[
E(\bar X) = \frac{1}{n}\sum_{k=1}\mu_k
\]</span></p>
</dd>
<dt>Variance</dt>
<dd><span class="math display">\[
\text{Var}(\bar X) = \frac{\sigma^2}{n}
\]</span>
</dd>
<dt>Unbiased Estimator of the Mean</dt>
<dd><p><span class="math inline">\(\bar X\)</span> is the unbiased estimator of the expectation <span class="math inline">\(\mu\)</span>.</p>
<p><span class="math display">\[
E(\bar X) = \mu
\]</span></p>
</dd>
</dl>
<h2 id="central-limit-theorem">Central Limit Theorem</h2>
<p>At the limit of large <span class="math inline">\(n\)</span>,</p>
<p><span class="math display">\[
\bar X \to \text{Normal}(\mu,~ \sigma^2/n)
\]</span></p>
<p><span class="math display">\[
P(\bar X \le x) \approx \Phi\left(\frac{x-\mu}{\sigma / \sqrt{n}}\right)
\]</span></p>
<h3 id="confidence-intervals">Confidence Intervals</h3>
<p>As <span class="math inline">\(\bar X\)</span> is most commonly useful for the unbiased estimator of the mean, thus it is also commonly used to generate the confidence interval of the estimate.</p>
<p>Let <span class="math inline">\(p\)</span> be the chance for <span class="math inline">\(X\)</span> to be in the confidence interval <span class="math inline">\(c\)</span> away from the mean <span class="math inline">\(\mu\)</span> :</p>
<p><span class="math display">\[
P(\bar X \in \mu \pm c) = P\left( \mu \in \bar X \pm c\right) = P\left( \abs{\bar X - \mu} \le c \right)
\]</span></p>
<ul>
<li>We take preference of the RHS for notation here on since it’s more algebraically useful.</li>
<li>The center expression is more accurate as it implies “the chance that the random interval <span class="math inline">\(\bar X \pm c\)</span> contains the mean <span class="math inline">\(\mu\)</span>”.</li>
<li>More often, the numerical value is used post-calculation. This value is called the <strong>p-value</strong></li>
</ul>
<p>At large <span class="math inline">\(n\)</span>, CLT holds so that</p>
<p><span class="math display">\[
\begin{align*}
p &amp;= P\left( \abs{\bar X - \mu} \le c \right) \\
&amp;= P(\bar X \le \mu + c) - P(\bar X \le \mu - c)\\
&amp;= \Phi\left(\frac{c}{\sigma/\sqrt{n}}\right) - \Phi\left(\frac{c-2\mu}{\sigma/\sqrt{n}}\right)
\end{align*}
\]</span></p>
<dl>
<dt>Number of SDs</dt>
<dd><p>More conveniently we can replace <span class="math inline">\(c\)</span> with the number <span class="math inline">\(z\)</span> of SDs away from the mean,</p>
<p><span class="math display">\[
p \equiv P\left( \abs{\bar X - \mu} \le z\frac{\sigma}{\sqrt{n}} \right); \qquad z = \frac{c}{\sigma/\sqrt{n}}
\]</span></p>
<p>The left bound of the interval is at <span class="math inline">\(\mu - z \cdot \sigma/\sqrt{n}\)</span> where we know the CDF to that bound is,</p>
<p><span class="math display">\[
\Phi\left(-z\right) = \frac{1-p}{2}
\]</span></p>
<p>To solve for <span class="math inline">\(z\)</span>, we need the inverse CDF of the standard normal,</p>
<p><span class="math display">\[
\boxed{z = \abs{\Phi^{-1}\left(\frac{1-p}{2}\right)}}
\]</span></p>
<p>Alternatively, we can use the right bound of the interval to get,</p>
<p><span class="math display">\[
\boxed{z = \Phi^{-1}\left(p + \frac{1-p}{2}\right)}
\]</span></p>
</dd>
</dl>
<h2 id="sample-average">Sample Average</h2>
<p>Now let <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> be an array of independent samples of the random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<h3 id="law-of-averages">Law of Averages</h3>
<p><span class="math display">\[
P(\abs{\bar X - \mu} &lt; \epsilon) \rightarrow 1, \quad \text{as } n \to \infty
\]</span></p>
<dl>
<dt>Proof</dt>
<dd><p>Since,</p>
<p><span class="math display">\[
P(\abs{\bar X - \mu} &lt; \epsilon) = 1 - P(\abs{\bar X - \mu} \ge \epsilon)
\]</span></p>
<p>By Chebyshev’s inequality,</p>
<p><span class="math display">\[
P(\abs{\bar X - \mu} \ge \epsilon) = \frac{\sigma^2}{n \epsilon^2} \to 0, \quad \text{as } n \to \infty
\]</span></p>
</dd>
</dl>
