<p>For some loss function <span class="math inline">\(L\)</span> the <strong>Bayes Decision Rule</strong> simply chooses the class with the maximum posterior distribution scaled by the loss of misclassification. For the cases of two classes, the decision rule looks like,</p>
<p><span class="math display">\[
\hat y(x) = \mathop{\arg\max}_A L(\neg A,A)P(A\mid x)
\]</span></p>
<p>A motivation why the sign is greater than is because we assign a greater loss if say class <span class="math inline">\(A\)</span> was classified wrong. If it was classified wrong, there should be a greater chance that the classifier will change its decision rule to choose classify <span class="math inline">\(A\)</span> better.</p>
<h2 id="zero-one-loss">Zero-One Loss</h2>
<p>The risk function for the 0-1 loss is,</p>
<p><span class="math display">\[
R(\hat y) = P(\hat y(x)\text{ is wrong})
\]</span></p>
<p>This suggest that the decision boundary is the set of all points where <span class="math inline">\(\set{x : P(A \mid x) = 0.5}\)</span></p>
