<p>From here on, we will deal with only single layer NN for simplicity.</p>
<p>The final loss function is given as <span class="math inline">\(\mathcal L (z,y)\)</span> where <span class="math inline">\(z = h(X_i)\)</span> and is a vector of size <span class="math inline">\(k\)</span>. <span class="math inline">\(h(X_i)\)</span> is treated as <span class="math inline">\(k\)</span> predictions of the label vector <span class="math inline">\(y\)</span> hence the loss function actually takes in <span class="math inline">\(Y \in \mathbb R^{n \times k}\)</span></p>
<p><span class="math display">\[
\mathcal L = \frac{1}{n}\sum_{i=1}^n L(h(X_i), Y_i )
\]</span></p>
<p>With the loss function established we begin the algorithm:</p>
<ol style="list-style-type: decimal">
<li>Set all weights to random in all transformation matrix <span class="math inline">\(T^{(\ell)}\)</span>. This also motivates an alias for <span class="math inline">\(T\)</span> which is the weight matrix (sometimes written as <span class="math inline">\(V,W\)</span> for one-layer NN)</li>
<li>Calculate the error <span class="math inline">\(\varepsilon\)</span></li>
<li>Gradient descent update to the weights.</li>
</ol>
<p>Now the hard part is computing the gradient. Naively one may compute the gradient for each weight (i.e., each edge) of the NN which would cost <span class="math inline">\(\Theta(\text{edges})^2\)</span>. The better idea is to update the edges as it goes through dynamic programming. Using this idea we get <strong>backpropagation</strong>.</p>
<h2 id="backpropagation">Backpropagation</h2>
<p>An update rule that cost <span class="math inline">\(\mathcal O(\text{edges})\)</span> to calculate the gradient. There's two parts of back pass which is forward pass and backward propagating.</p>
<p>We simplify the problem by doing stochastic gradient descent on one sample point. The goal is to solve for,</p>
<p><span class="math display">\[
\nabla_{T^{(\ell)}} L
\]</span></p>
<p>Let's start with <span class="math inline">\(T^{(1)}\)</span> which is an input of both <span class="math inline">\(h\)</span> thus also <span class="math inline">\(z\)</span> since <span class="math inline">\(L \circ (z, y) \circ (h, T^{(2)}) \circ (x, T^{(1)})\)</span>.</p>
<p>For each row or node <span class="math inline">\(T_i^{(1)}\)</span> <span class="math display">\[
\nabla_{T_i^{(1)}}L = \underbrace{\frac{\partial L}{\partial h_i}}_\text{back pass} \overbrace{\nabla_{T_i^{(1)}} h_i}^\text{foward pass}
\]</span></p>
