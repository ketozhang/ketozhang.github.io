<p>The vanishing gradient problem is an effect of unit saturation. This occurs when most of the training outputs of the activation function <span class="math inline">\(\hat y\)</span> is close to region of zero gradient.</p>
<p>For instance if <span class="math inline">\(\hat y\)</span> is the sigmoid function, then unit saturation occurs when <span class="math inline">\(\hat y\)</span> is close to 0 or 1.</p>
<p>Being in zero curvature region causes gradient descent to get stuck since the step size becomes too small. Hence appropriately named the <strong>vanishing gradient problem</strong>.</p>
<h2 id="solutions">Solutions</h2>
<p>Solutions for mitigating this effect includes:</p>
<ol style="list-style-type: decimal">
<li><p>Initialization: Set weights of edges to random with,</p>
<span class="math display">\[
\begin{gather}
\mu = 0\\
\sigma = \frac{1}{\sqrt{n}}
\end{gather}
\]</span></li>
<li>Set the target output values to values with high curvature.</li>
<li>Change the gradient by some amount such that the parameter are no longer moving towards steepest descent.</li>
<li>Use different loss function like cross-entropy.</li>
<li><p>Use a different activation function. A famous replacement for sigmoid is the rectified linear unit (ReLU).</p></li>
</ol>
