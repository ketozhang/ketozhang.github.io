<p><strong>k-Nearest Neighbors</strong> (<strong>k-NN</strong>) is used in both classification and regression. The predictor predicts the aggregate of the <span class="math inline">\(k\)</span> nearest training label using some distance metric <span class="math inline">\(d(z,x)\)</span>.</p>
<p>Specifically for classification, the mode of the k-NN is often used as the predicted label, while for regression, the mean of the k-NN is often used.</p>
<h2 id="algorithm">Algorithm</h2>
<p>Given a query point <span class="math inline">\(z\)</span>:</p>
<ul>
<li>Scan through all <span class="math inline">\(n\)</span> sample points.</li>
<li>Add neighbors to <span class="math inline">\(k\)</span> size max-heap.</li>
</ul>
<p>The computation time is <span class="math inline">\(\mathcal O(nd + n \log k)\)</span></p>
<h2 id="low-dimension-algorithm">Low Dimension Algorithm</h2>
<ul>
<li>In very small dimensions (<span class="math inline">\(~2-5\)</span>) Use Voronoi diagrams.</li>
<li>In medium dimensions (<span class="math inline">\(\lesssim30\)</span>) Use k-d trees.</li>
<li>Otherwise use traditional algorithm or preprocessing.</li>
</ul>
<h3 id="vornoi-diagrams">Vornoi Diagrams</h3>
<p>Let <span class="math inline">\(X\)</span> be a point set. The Voronoi cell of <span class="math inline">\(w \in X\)</span> is</p>
<p><span class="math display">\[\text{Vor} w = \{ |\overline{pw}| \le |\overline{pv}| \forall v \in X \}\]</span></p>
<p>For <span class="math inline">\(k=1\)</span>, * In 2D, using the Vornoi diagram and trapezoidal map for point location the k-NN can be reduced to <span class="math inline">\(\mathcal O(\log n)\)</span> * In <span class="math inline">\(\dim d\)</span>, use binary space partition tree for point location.</p>
<p>Other than that, the Vornoi diagram isnâ€™t feasible for k-NN.</p>
<h3 id="k-d-trees">k-D Trees</h3>
<p>Very similar to decision trees with the follow differences:</p>
<ul>
<li>Choose splitting feature is chosen by the feature of greatest width which maximizes <span class="math inline">\(X_{ji} - X_{ki}\)</span>.
<ul>
<li>Alternatively rotate the features.</li>
</ul></li>
<li>Choose splitting value by either:
<ul>
<li>Median (shortest tree builds in <span class="math inline">\(\mathcal O(nd \log n)\)</span> )</li>
<li>Pairwise midpoint (builds in <span class="math inline">\(\mathcal O(n \log n)\)</span> )</li>
</ul></li>
<li>Each internal node stores a sample point at the split.</li>
<li>Each subtree can be seen as a box region with values of its node and sub-nodes.</li>
</ul>
<ol type="1">
<li><p>Given a query point <span class="math inline">\(z\)</span> ,find a sample point such that <span class="math inline">\(|\overline{zw}| \le (1 + \epsilon)|\overline{zs}|\)</span> where <span class="math inline">\(s\)</span> is the true nearest neighbor. The algorithm maintains:</p>
<ul>
<li>Nearest neighbor found so far</li>
<li>Binary heap of unexplored subtrees, keyed by distance from the query.</li>
</ul></li>
<li><p>A better nearest point is obtained by going through the binary heap and exploring the subtree no further than the distance to the nearest neighbor so far.</p></li>
</ol>
<h2 id="facts-and-heuristics">Facts and Heuristics</h2>
<dl>
<dt>Theorem (Cover &amp; Hart 1967)</dt>
<dd><p>As <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(k=1\)</span> has the error rate bounded by <span class="math inline">\(&lt; B(2-B)\)</span> where <span class="math inline">\(B\)</span> is the Bayes risk.</p>
<p>If <span class="math inline">\(k=2\)</span>, then error is <span class="math inline">\(\le 2B(1-B)\)</span>.</p>
</dd>
<dt>Theorem (Fix &amp; Hodges 1951)</dt>
<dd>As <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(k \rightarrow \infty\)</span>, and <span class="math inline">\(\frac{k}{n} \rightarrow 0\)</span> then k-NN error rate converges to <span class="math inline">\(B\)</span> the Bayes risk.
</dd>
</dl>
