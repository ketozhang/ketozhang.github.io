<p>The ridge regression is regression with the <span class="math inline">\(L_2\)</span> regularization, <span class="math display">\[
\mathcal{L} = \sum_{i=1}^n L(y,\hat y) + \lambda|\theta|^2
\]</span></p>
<ul>
<li><span class="math inline">\(\theta\)</span> : The parameter vector with the bias parameter as zero instead of one <span class="math inline">\(\begin{bmatrix} 0 &amp; \theta_1 &amp; \ldots &amp; \theta_d\end{bmatrix}\)</span> to prevent penalizing the bias term.</li>
</ul>
<p>For simplicity, here one out we will ignore the bias term.</p>
<p>There are very strong properties of ridge regression that makes this regularization favorable:</p>
<ol style="list-style-type: decimal">
<li>Guarantees a unique solution because <span class="math inline">\(\mathcal L(\theta)\)</span> is always convex even if <span class="math inline">\(L(\theta)\)</span> is not (for least-square it is).</li>
<li>Reduces overfitting by penalizing large parameters.</li>
</ol>
<h2 id="positive-definite-normal-equation">Positive Definite Normal Equation</h2>
<p>The addition of the <span class="math inline">\(L_2\)</span> regularization guarantees that <span class="math inline">\(\mathcal L(\theta)\)</span> is convex simply because the matrices in the normal equation are guaranteed to be positive definite.</p>
<p>Let's take for example the least-squares loss, <span class="math display">\[
\mathcal L(\theta) = \sum_{i=1}^n{|y - X\theta|^2  +\lambda|\theta|^2}
\]</span> The normal equation is given by, <span class="math display">\[
\begin{gather}
(X^\top X + \lambda I)\theta = X^\top y\\
\theta^* = \frac{2|y-X\theta^*|^2}{\lambda} X^\top \left({X\theta^* - y}\right)
\end{gather}
\]</span></p>
<h2 id="relationship-to-map">Relationship to MAP</h2>
<p>The addition of the <span class="math inline">\(L_2\)</span> regularization is equivalent of turning an MLE problem to an MAP problem (i.e., by adding a prior).</p>
<p>Once again we use the least-squares loss as an example. The least-squares loss is the log-likelihood function for the Gaussian likelihood. We simply attempt to write the log-posterior problem with this likelihood, <span class="math display">\[
\begin{align}
\theta^* &amp;= \mathop{\arg\min}_\theta \sum_{i=1}^n|y + X\theta|^2 - \lambda |\theta|^2\\
\theta^* &amp;= \mathop{\arg\max}_\theta \left[-\sum_{i=1}^n|y - X\theta|^2 - \lambda |\theta|^2\right]
\end{align}
\]</span> Thus the prior <span class="math inline">\(P(\theta) \propto \lambda |\theta|^2\)</span>.</p>
