<!doctype html>
<html lang='en'>

<head>
    <meta charset="UTF-8">
    <title>StaticPy</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap time -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link href="https://ketozhang.github.io/StaticPy/static/base.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Code+Pro|Quicksand" rel="stylesheet">
    <link href="https://ketozhang.github.io/StaticPy/static/prism.css" rel="stylesheet">
    <link rel="shortcut icon" href="https://ketozhang.github.io/StaticPy/static/favicon.ico">

    <!-- LaTeX Stylesheet -->
    
    <div class="d-none">
    $$
        \newcommand{abs}[1]{\left | #1 \right |}
        \newcommand{set}[1]{\left\{#1\right\}}
    $$
    </div>
    
</head>

<body>
    
<div class="container-fluid" id="note">
    <h1 class="title text-center pt-3">Gradient Descent</h1>
    <hr>
    <div class="row">
        <div class="col-lg-2 d-none d-lg-block"></div>
        <div class="col-lg-7 col-md-10">
            <p>Gradient descent is a iteratative optimizer that attempts to minimize the problem with only the information of the gradient. Here we will use it to minimize some loss function over the parameter space given by <span class="math inline">\(\theta\)</span>. Recall that this gradient for this is,</p>
<p><span class="math display">\[
\nabla \mathcal{L}(\theta) =  \left(\frac{\partial \mathcal{L}}{\partial \theta_1}, \frac{\partial \mathcal{L}}{\partial \theta_2}, \ldots \frac{\partial \mathcal{L}}{\partial \theta_n} \right)
\]</span></p>
<p>In numerical analysis, the <strong>gradient descent</strong> method is:</p>
<ol style="list-style-type: decimal">
<li>Take the gradient at some point along your curve <span class="math inline">\(\mathcal{L}(\theta)\)</span>. This point is denoted as <span class="math inline">\(\theta^{(0)}\)</span></li>
<li>The new point is taken to be the current point <span class="math inline">\(\theta^{(0)}\)</span> minus a constant factor of <span class="math inline">\(\nabla \mathcal{L}(\theta^{(t)})\)</span></li>
</ol>
<p><span class="math display">\[\boxed{ \theta^{(t+1)} = \theta^{(t)} - \alpha\nabla \mathcal{L}(\theta^{(t)}) }\]</span></p>
<p>Another way of writing this is by the order of approximation notation. We say that the <span class="math inline">\(n\)</span>-th order approximation for the vector <span class="math inline">\(x\)</span> that minimizes <span class="math inline">\(\mathcal L(x)\)</span> is <span class="math inline">\(x^{(n)}\)</span> where,</p>
<p><span class="math display">\[\boxed{ x^{(n)} =  x^{(0)} - \alpha_1 \nabla \mathcal L(x^{(0)}) - \alpha_2 \nabla \mathcal L(x^{(1)}) - \ldots - \alpha_n\nabla \mathcal L(x^{(n-1)})}\]</span></p>
<ul>
<li><span class="math inline">\(x^{0}\)</span> : Initial guess minimum</li>
<li><span class="math inline">\(\alpha_1 \ldots \alpha_n\)</span> : A constant positive optimization parameter that you choose to say how sensitive should the next order approximation be. Often times <span class="math inline">\(\alpha_1 = \ldots \alpha_n\)</span> and this works well numerically sufficiently small value.</li>
</ul>
<hr />
<ul>
<li>Computation time: <span class="math display">\[\mathcal O(nd)\]</span>
<ul>
<li><span class="math inline">\(n\)</span> : Number of training points</li>
<li><span class="math inline">\(d\)</span> : Dimensions of training points</li>
</ul></li>
</ul>
<h2 id="stocastic-gradient-descent-sgd">Stocastic Gradient Descent (SGD)</h2>
<p>The <strong>stocastic gradient descent (SGD)</strong> is a class of methods of using only a smaller sample of the training points chosen by a random process.</p>
<ul>
<li><p>Computation cost:</p>
<p><span class="math display">\[ \mathcal O(n&#39;d) \]</span></p>
<ul>
<li><span class="math inline">\(n&#39;\)</span> : Number of sample points. This number is often an order or more magnitudes less than <span class="math inline">\(n\)</span>.</li>
</ul></li>
</ul>
        </div>
        <div class="col-md-2 d-none d-md-block">
            <div class="sticky-top onthispage">
                <ul id="onthispage-list" class="no-list-style">On This Page</ul>
            </div>
        </div>
    </div>
</div>

    <footer class="py-2 bg-white">
    <div class="container">
        <p class="text-right m-0"> Generated by <a href="https://github.com/ketozhang/StaticPy">StaticPy</a> |
            Designed
            by <a href="https://github.com/ketozhang">Keto Z.</a>
        </p>
    </div>
</footer>

    <!-- Scripts -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>

    <script src="https://ketozhang.github.io/StaticPy/static/datetime.js"></script>
    <script src="https://ketozhang.github.io/StaticPy/static/codeHighlight.js"></script>
    <script src="https://ketozhang.github.io/StaticPy/static/prism.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.js"></script>
    <script src="https://ketozhang.github.io/StaticPy/static/onThisPage.js"></script>

    <!-- MathJax -->
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
    </script>
    

    <!-- Bootstrap JS -->
    <!-- TODO: trim JS if not needed -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>

    <!-- React -->
    <!-- TODO: do we need React? -->
    <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react-dom.min.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/react/0.13.3/JSXTransformer.js"></script>-->


    <script>
        anchors.add();
        $(document).ready(function () {
            Prism.highlightAll();
        })
    </script>
</body>

</html>