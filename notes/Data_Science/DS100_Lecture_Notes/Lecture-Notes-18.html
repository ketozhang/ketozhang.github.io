<!doctype html>
<html lang='en'>

<head>
    <meta charset="UTF-8">
    <title>StaticPy</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap time -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link href="https://ketozhang.github.io/StaticPy/static/base.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Code+Pro|Quicksand" rel="stylesheet">
    <link href="https://ketozhang.github.io/StaticPy/static/prism.css" rel="stylesheet">
    <link rel="shortcut icon" href="https://ketozhang.github.io/StaticPy/static/favicon.ico">

    <!-- LaTeX Stylesheet -->
    
    <div class="d-none">
    $$
        \newcommand{abs}[1]{\left | #1 \right |}
        \newcommand{set}[1]{\left\{#1\right\}}
    $$
    </div>
    
</head>

<body>
    
<div class="container-fluid" id="note">
    <h1 class="title text-center pt-3">Lecture Notes18</h1>
    <hr>
    <div class="row">
        <div class="col-lg-2 d-none d-lg-block"></div>
        <div class="col-lg-7 col-md-10">
            <h1 id="lecture-18---the-bias-variane-tradeoff-and-regularization">Lecture 18 - The Bias Variane Tradeoff and Regularization</h1>
<h2 id="linear-model-for-non-linear-data">Linear Model for Non-Linear Data</h2>
<p>Say we wish to model a general function <span class="math inline">\(f(x)\)</span> which is not always linear, a linear fit to <span class="math inline">\(f(x)\)</span> can be generally approximated like so:</p>
<p><span class="math display">\[ f_\theta(\phi(x)) = \phi(x)^T \theta = \sum_{j=1}^{k}{\phi(x)_j \theta_j}  \]</span></p>
<ul>
<li><span class="math inline">\(\phi(x)\)</span> : A vector of functions.
<ul>
<li>If you're familiar with linear algebra, this is similar to a set of basis vectors but not the same.</li>
</ul></li>
<li><span class="math inline">\(\theta\)</span> : The model's parameter that weighs each vector hence it's common to call this the weight coefficient.</li>
</ul>
<dl>
<dt>Bias</dt>
<dd><p>The <strong>bias</strong> is the expected deviation between the predicted value and the true value.</p>
<p>Depends on:</p>
<ul>
<li>choice of model function <span class="math inline">\(f(\theta)\)</span></li>
<li>learning procedure</li>
</ul>
</dd>
<dt>Observation Variance</dt>
<dd><p>Observation variance is the variability of the random noise in the process we are trying to model. It is usually impossible to get rid of all observation variance. Sources of observation variance comes from:</p>
<ul>
<li>measurement variability</li>
<li>stochasticity</li>
<li>missing information</li>
</ul>
</dd>
<dt>Estimated Model Variance</dt>
<dd><p><strong>Estimated model variance</strong> is the variability in the predicted value accros different training datasets</p>
<ul>
<li>Sensitive to variation in the training data</li>
<li>Poor generalization</li>
<li>Overfitting</li>
</ul>
</dd>
</dl>
<h2 id="example-analysis-of-squared-error">Example: Analysis of Squared Error</h2>
<p>Let's say we wish to model our data <span class="math inline">\(y\)</span> that has some noise. We will absorb this noise into an error value <span class="math inline">\(\epsilon\)</span> such that the true function that describe the system is denoted as <span class="math inline">\(h(x)\)</span> therefore the data is,</p>
<p><span class="math display">\[ y =h(x) + \epsilon \]</span></p>
<dl>
<dt>Expected Loss</dt>
<dd>Before we compute the expected loss notice that the <span class="math inline">\(\Expected{y} = \Expected{h(x)}\)</span> since <span class="math inline">\(\Expected{\epsilon} = 0\)</span> due to the error being a random variable.
</dd>
</dl>
<p><span class="math display">\[
\begin{align*}
\Expected{(y-f_{\hat\theta})^2} &amp;= \Expected{(y- h(x) + h(x) - f_{\hat\theta})^2}\\
&amp;= \Expected{(y-h(x))^2} + \Expected{(h(x)-f_{\hat\theta})^2} +  2\Expected{(y-h(x))(h(x)-f_{\hat\theta})}\\
\Expected{(y-f_{\hat\theta})^2} &amp;= \underbrace{\Expected{(h(x)-f_{\hat\theta})^2}}_\text{Obs. Variance} + (\underbrace{h(x)-\Expected{f_{\hat\theta}^2}}_\text{Bias})^2 + \underbrace{\Expected{(\Expected{f_{\hat\theta}} - f_{\hat\theta})^2}}_\text{Model Variance}
\end{align*}
\]</span></p>
<h2 id="regularization">Regularization</h2>
<p>Regularization is to add a parameter <span class="math inline">\(\lambda\)</span> to control them modeling complexity through the regularization function <span class="math inline">\(R(\theta)\)</span>. This is used in determining the model parameter.</p>
<p><span class="math display">\[\hat \theta \equiv \arg \min \frac{1}{n}\sum_i^n{\ell(y_i,f_\theta(x_i))} + \lambda \mathbf{R}(\theta)\]</span></p>
<p>Example of regularization include the same exact function as the loss functions <span class="math inline">\(\Bbb{L^1}\)</span> (Ridge regularization) and <span class="math inline">\(\Bbb{L^2}\)</span> (LASSO regularization).</p>
<p>The point is to simply introduce another function that further optimizes the model parameter.</p>
        </div>
        <div class="col-md-2 d-none d-md-block">
            <div class="sticky-top onthispage">
                <ul id="onthispage-list" class="no-list-style">On This Page</ul>
            </div>
        </div>
    </div>
</div>

    <footer class="py-2 bg-white">
    <div class="container">
        <p class="text-right m-0"> Generated by <a href="https://github.com/ketozhang/StaticPy">StaticPy</a> |
            Designed
            by <a href="https://github.com/ketozhang">Keto Z.</a>
        </p>
    </div>
</footer>

    <!-- Scripts -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>

    <script src="https://ketozhang.github.io/StaticPy/static/datetime.js"></script>
    <script src="https://ketozhang.github.io/StaticPy/static/codeHighlight.js"></script>
    <script src="https://ketozhang.github.io/StaticPy/static/prism.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.js"></script>
    <script src="https://ketozhang.github.io/StaticPy/static/onThisPage.js"></script>

    <!-- MathJax -->
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
    </script>
    

    <!-- Bootstrap JS -->
    <!-- TODO: trim JS if not needed -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>

    <!-- React -->
    <!-- TODO: do we need React? -->
    <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react-dom.min.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/react/0.13.3/JSXTransformer.js"></script>-->


    <script>
        anchors.add();
        $(document).ready(function () {
            Prism.highlightAll();
        })
    </script>
</body>

</html>