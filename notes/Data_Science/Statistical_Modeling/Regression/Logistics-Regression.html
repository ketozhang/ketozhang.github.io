<!doctype html>
<html lang='en'>

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-145024207-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-145024207-1');
    </script>
    <meta charset="UTF-8">
    <title>Keto</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap time -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link href="https://ketozhang.github.io/static/base.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Code+Pro|Quicksand" rel="stylesheet">
    <link href="https://ketozhang.github.io/static/prism.css" rel="stylesheet">
    <link rel="shortcut icon" href="https://ketozhang.github.io/static/favicon.ico">

    <!-- LaTeX Stylesheet -->
    
    <div class="d-none">
        $$
        \newcommand{abs}[1]{\left | #1 \right |}
        \newcommand{set}[1]{\left\{#1\right\}}
    $$
    </div>
    
</head>

<body>
    
<div class="container-fluid" id="note">
    <!-- <nav class="navbar navbar-expand-lg">
        <button id="sidenavCollapse" class="btn btn-info" type="button">
            <i class="fas fa-align-left"></i>
            <span>Toggle Sidebar</span>
        </button>
    </nav> -->
    <h1 class="title text-center pt-3">Logistics Regression</h1>
    <hr>
    <div class="row">
        <!-- Notebook Sidenav -->
        <div class="col-xl-2 col-lg-3 col-3 d-none d-md-block bg-light pt-1">
            <div class="sidenav sticky-top vh-80">
                <div class="sidenav-title row no-gutters">
                    <div class="col-9">
                        <strong>Logistics Regression</strong>
                    </div>
                    <div class="col-2 offset-1">
                        <a class="go-up" href="https://ketozhang.github.io/notes/Data_Science/Statistical_Modeling/Regression/" role="button">
                            <img src="https://img.icons8.com/dotty/80/000000/up.png" style="width: 25px; height: 25px">
                        </a>
                    </div>
                </div>
                <div class="sidenav-body no-gutters">
                    <ul class="no-list-style pl-3">
                        
                    </ul>
                </div>
            </div>
        </div>

        <!-- Content -->
        <div class="col-xl-8 col-lg-6 col-9 h-100">
            
            <div class="markdown-content">
                <p>The <strong>logistic regression</strong> transforms the linear regression problem with the <strong>logistic</strong> or <strong>sigmoid function</strong> <span class="math inline">\(s(z)\)</span> used for posterior distribution estimation and classification<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p>We will motivate this with a binary problem where <span class="math inline">\(Y: \{0,1\}\)</span> notice that the logistic function transforms the linear regression to the interval <span class="math inline">\([0,1]\)</span>. <span class="math display">\[
\begin{gather}
P(X \mid Y=\hat y) = s(\hat y); \qquad \hat y  = X\theta \\
s(z) = \frac{e^z}{1+e^{z}} = \frac{1}{1+e^{-z}}; \qquad s(z) \in [0,1]
\end{gather}
\]</span> * <span class="math inline">\(\hat y\)</span> : Model</p>
<p>This is purposely done so that we can interpret the logistic regression as a probability of classification given the data <span class="math inline">\(P(X \mid Y)\)</span>. From here on out we will choose to classify if <span class="math inline">\(Y=1\)</span>.</p>
<p>A more insight into what we’re modeling with <span class="math inline">\(\hat y\)</span> is to notice the following identity <span class="math display">\[
\begin{equation}
\log \left[\frac{P(X \mid Y=1)}{1 - P(X\mid Y=1)}\right] = \hat y
\end{equation}
\]</span> This identity is called <strong>log-odds</strong> or <strong>logit</strong> hence we’re fitting some model to the log ratio of probabilities that <span class="math inline">\(Y=1\)</span> compared to <span class="math inline">\(Y=0\)</span>.</p>
<p>The objective function to solve for <span class="math inline">\(\theta\)</span> is chosen to be the maximum likelihood (we will discuss why this is equivalent to minimum loss later). The likelihood function is naturally the binomial distribution. We will disregard the leading coefficient because it will be irrelevant in maximization. The decision rule will be discussion later in the <a href="#Decision_Rule">Decision Rule</a> section. <span class="math display">\[
\begin{align*}
\log P(X \mid Y=1) &amp;= \sum_{i: y_i = 1} \log P(X_i \mid Y=1~;~\theta) + \sum_{i : y_i = 0} \log \left[1 - P(X_i \mid Y=1~;~\theta)\right]\\
&amp;= \sum_{i=1}^n y_i\log P(X_i \mid Y=1~;~\theta) + (1-y_i) \log \left[1 - P(X_i \mid Y=1~;~\theta)\right]
\end{align*}
\]</span> We can write this out more compactly by plugging substituting the log-likelihood for each data point with the logistic function where <span class="math inline">\(s_i = s(\hat y)\)</span>, <span class="math display">\[
\log P(X \mid Y=1) = \sum_{i=1}^n y_i \log s_i + (1-y_i) \log (1 - s_i)
\]</span> The objective function is then, <span class="math display">\[
\theta^* = \mathop{\arg\max}_{\theta} \bigg[\log P(X \mid Y=1) \bigg] \label{eq:objective}
\]</span></p>
<p>The solution to this is discussion in the <a href="#solution">Solution</a> section.</p>
<h2 id="solutions">Solutions</h2>
<p>The gradient for the objective function <span class="math inline">\(\eqref{eq:objective}\)</span> (let’s denote it as <span class="math inline">\(\log P\)</span> for brevity is, <span class="math display">\[
\nabla_\theta \log P = -X^\top(y-s) = 0
\]</span> * <span class="math inline">\(y\)</span> : Vector of labels * <span class="math inline">\(s\)</span>: Vector of the sigmoid for each data point <span class="math inline">\(s = [s_1 \quad s_2 \quad \ldots \quad s_n]^\top\)</span></p>
<p>Hence, the normal equation is, <span class="math display">\[
X^\top y = X^\top s
\]</span> Unfortunately there is no analytical solution that solves for <span class="math inline">\(\theta^*\)</span>. Numerical solutions can be estimated with the methods below:</p>
<ol type="1">
<li>Newton’s Method</li>
<li>Gradient Descent</li>
</ol>
<p>The Newton’s method is well behaved since the Hessian is easily calculated: <span class="math display">\[
H_\theta(\log P) = X^\top \Omega X
\]</span> Where <span class="math inline">\(\Omega \in \mathbb R^{d \times d}\)</span> is a diagonal matrix with entries <span class="math inline">\(\omega_i = s_i(1-s_i)\)</span>. Using the gradient and the Hessian, Newton’s method approximates the next step <span class="math inline">\(\theta(t+1)\)</span> given the current step <span class="math inline">\(\theta(t)\)</span> as, <span class="math display">\[
\begin{gather*}
\theta(t+1) = \theta(t) + \Delta \theta\\
[X^\top \Omega X]\Delta \theta = -X^\top(y-s)
\end{gather*}
\]</span> Unfortunately the solution is not well behaved if <span class="math inline">\(X^\top X​\)</span> is not always positive definite. However, a ridge regularization discussed in the section <a href="#regularization">Regularization</a> can guarantee this.</p>
<h2 id="decision-rule">Decision Rule</h2>
<p>The decision rule is simple following the Bayes optimal rule: <span class="math display">\[
\hat Y = \begin{cases}
1 &amp; P(X \mid Y=1) &gt; 0.5\\
0 &amp; \text{otherwise.}
\end{cases}
\]</span></p>
<h2 id="regularization">Regularization</h2>
<p>The most practical regularization is to add the <span class="math inline">\(L_2\)</span> (Ridge) regularization which guarantees that Newton’s method is solvable by inversion because the Hessian is positive definite. <span class="math display">\[
\begin{gather*}
H_\theta(\log P) = X^\top \Omega X + 2\lambda I\\
\Delta \theta = -X^\top(y-s)[X^\top \Omega X + 2\lambda I]^{-1}
\end{gather*}
\]</span></p>
<h2 id="multiclass-generalization">Multiclass Generalization</h2>
<h2 id="equivalence-to-loss-function">Equivalence to Loss Function</h2>
<p>The logistics regression is linear regression with a logistic loss function. We motivate using logistic loss with the example of data that comes from two classes <span class="math inline">\(y: {0,1}\)</span>. The probability that we receive <span class="math inline">\(y=0\)</span> along with some data <span class="math inline">\(x\)</span> is <span class="math inline">\(P(y=0 \mid x)\)</span>. The probability of the other class is then ​<span class="math inline">\(1 - P(y=0 \mid x)\)</span>. <span class="math display">\[
P(X \mid \hat y) = \sum_{i=1}^n  \bigg[ \log P(y_i \mid X_i) + \log\Big(1-P(y_i \mid X_i)\Big) \bigg]
\]</span></p>
<p>Given <span class="math inline">\(y \in (0,1)\)</span> the logistics function is a discriminant model</p>
<p>The optimization problem is then,</p>
<p><span class="math display">\[
\hat y = \mathop{\arg\min} \sum_{i=1}^n - \left[ y_i \log s_i + (1-y_i)\log(1-s_i)\right]
\]</span></p>
<ul>
<li><p><span class="math inline">\(s_i\)</span> : The logistic function for row <span class="math inline">\(i\)</span>,</p>
<p><span class="math display">\[s_i(X_i^\top \theta) = \frac{1}{1+e^{-X_i^T \theta}}\]</span></p>
<p>Notice its derivative is,</p>
<p>$$s_i’ = s_i(1-s_i)​</p></li>
</ul>
<h2 id="properties">Properties</h2>
<ul>
<li>Differs from LDA or QDA by emphasizing the decision boundary</li>
<li>Less sensitive to outliers</li>
<li>Robust on non-Gaussians. (LDA is better if data is near Gaussian)</li>
<li>Unstable to new points for well separated classes.</li>
</ul>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>It’s unfortunate that it’s called logistics regression, but the fact that logistic regression does estimate a posterior distribution is why it’s justifiable.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</section>

            </div>
            
        </div>

        <!-- On This Page -->
        <div class="col-xl-2 col-lg-3 d-none d-lg-block">
            <div class="onthispage sidenav sticky-top vh-80">
                <strong class="sidenav-title">On This Page</strong>
                <div class="sidenav-body">
                    <ul id="onthispage-list" class="no-list-style"></ul>
                </div>
            </div>
        </div>

    <!-- Disqus Comments -->
    </div>
    
    <div class="row  mt-5">
        <div class="col-xl-8 col-lg-6 col-9 offset-xl-2 offset-lg-3">
            <div id="disqus_thread"></div>
            <script>

                /**
                *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
                *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
                var disqus_config = function () {
                    this.page.url = "";  // Replace PAGE_URL with your page's canonical URL variable
                    this.page.identifier = "/notes/Data_Science/Statistical_Modeling/Regression/Logistics-Regression.html"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
                };
                (function () { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://ketozhang.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                })();
            </script>
            <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered
                    by Disqus.</a></noscript>
            
        </div>
    </div>
</div>

    <footer class="py-2 bg-white">
    <div class="container">
        <p class="text-right m-0"> Generated by <a href="https://github.com/ketozhang/StaticPy">StaticPy</a> |
            Designed
            by <a href="https://github.com/ketozhang">Keto Z.</a>
        </p>
    </div>
</footer>

    <!-- Scripts -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>

    <script src="https://ketozhang.github.io/static/datetime.js"></script>
    <script src="https://ketozhang.github.io/static/sidenav.js"></script>
    <script src="https://ketozhang.github.io/static/codeHighlight.js"></script>
    <script src="https://ketozhang.github.io/static/prism.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.js"></script>
    <script src="https://ketozhang.github.io/static/onThisPage.js"></script>

    <!-- MathJax -->
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
    </script>
    

    <!-- Bootstrap JS -->
    <!-- TODO: trim JS if not needed -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>

    <!-- React -->
    <!-- TODO: do we need React? -->
    <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react-dom.min.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/react/0.13.3/JSXTransformer.js"></script>-->


    <script>
        anchors.add();
        $(document).ready(function () {
            Prism.highlightAll();
        })
    </script>
</body>

</html>