<!doctype html>
<html lang='en'>

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-145024207-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-145024207-1');
  </script>
  <meta charset="UTF-8">
  <title>
    
    Keto | Linear Regression
    
  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap time -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
  <!-- <link href="https://ketozhang.github.io/static/base.css" rel="stylesheet"> -->
  <link href="/static/main.css?3ea8c582" rel="stylesheet"">
  <link href=" https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400;500;600;700&display=swap"
    rel="stylesheet">
  <link href=" https://fonts.googleapis.com/css?family=Roboto|Source+Code+Pro" rel="stylesheet">
  <link href="https://ketozhang.github.io/static/prism.css" rel="stylesheet">
  <link rel="shortcut icon" href="https://ketozhang.github.io/static/favicon.ico">

  <!-- LaTeX Stylesheet -->
  
  <div class="d-none">
    $$
    \newcommand{d}{\mathrm d}
    \newcommand{avg}[1]{\expval{#1}}
        \renewcommand{vec}[1]{\vectorbold{#1}}
        $$
    </div>
    
</head>

<body>
    
<div class="container-fluid" id="note">
  <!-- <nav class="navbar navbar-expand-lg">
        <button id="sidenavCollapse" class="btn btn-info" type="button">
            <i class="fas fa-align-left"></i>
            <span>Toggle Sidebar</span>
        </button>
    </nav> -->
  <div id="note-body" class="row">
    <!-- Notebook Sidenav -->
    <div class="col-3 bg-light pt-1">
      <div class="sidenav sticky-top vh-80">
        <div class="sidenav-header row no-gutters">
          <div class="col-9">
            <strong>Linear Regression</strong>
          </div>
          <div class="col-2 offset-1">
            <a class="go-up" href="https://ketozhang.github.io/notes/Data_Science/Statistical_Modeling/Regression/" role="button">
              <img src="https://img.icons8.com/dotty/80/000000/up.png" style="width: 25px; height: 25px" />
            </a>
          </div>
        </div>
        <div class="sidenav-body no-gutters">
          <ul class="no-list-style pl-3">
            
          </ul>
        </div>
      </div>
    </div>

    <!-- Content -->
    <div class="col-7 col-md-6">
      <h1 class="title text-center pt-3">Linear Regression</h1>
      
      <div class="markdown-content">
        <p>Consider a dataset as a matrix of <span class="math inline">\(X \in \mathbb{R}^{n\times d+1}\)</span> where we’ve extracted <span class="math inline">\(n\)</span> numbers of data records/rows and <span class="math inline">\(d\)</span> numbers of feature columns along with one extra column for the intercept. For each row, we have a measurement or response <span class="math inline">\(y \in \mathbb R^{n \times 1}\)</span></p>
<p><span class="math display">\[ \hat y = f(X) \theta \]</span></p>
<ul>
<li><span class="math inline">\(\hat y\)</span> : Length <span class="math inline">\(n\)</span> vector of dependent values</li>
<li><span class="math inline">\(X\)</span> : The feature matrix of dimension <span class="math inline">\(d+1\)</span> where <span class="math inline">\(d\)</span> is the number of features.</li>
<li><span class="math inline">\(f(X)\)</span> : Optional function applied to the feature matrix. This is sometimes referred to as the design matrix otherwise <span class="math inline">\(X\)</span> is sometimes also called the design matrix.</li>
</ul>
<h2 id="motivation-of-linear-regression-through-probability">Motivation of Linear Regression Through Probability</h2>
<p>Whenever you see the statement of the confidence interval and/or the p-value, it comes from probability theory. The foundation of applying linear regression to empirical data comes from the theory of linear regression applied to random variables. That must mean there is an underlying assumption of probability in our data. (See <a href="../Probabilistic_Models/Probabilistic_Linear_Regression">Probabilistic Linear Regression</a>).</p>
<p>The most common assumption is can be stated as the empirical data we’ve taken comprise of signal (the truth value) and random noise (the error). Once again, the randomness is in its error (this error may be natural or artificial/empirical error). We look at the world and measure <span class="math inline">\(y\)</span> knowing that it’s not always truth but instead,</p>
<p><span class="math display">\[
y = y^* + \varepsilon
\]</span></p>
<p>where <span class="math inline">\(y^*\)</span> is the truth and <span class="math inline">\(\varepsilon\)</span> is a random variable representing the error.</p>
<p>Therefore, <strong>the underlying assumption of our world is that the data contains randomness and that randomness is in its error</strong>.</p>
<h2 id="the-assumption-of-error">The Assumption of Error</h2>
<p>The most common assumption for the error is:</p>
<ul>
<li>a random vector</li>
<li>distributed as <span class="math inline">\(\text{Normal}(\vec 0, \sigma^2)\)</span> where <span class="math inline">\(\sigma^2 = (\sigma_1^2, \sigma_2^2, \ldots, \sigma_n^2)\)</span></li>
</ul>
<p>That is to say the error for every measurement is normally distributed (aka gaussian white noise), independent from each other.</p>
<h2 id="ordinary-least-squares">Ordinary Least Squares</h2>
<p>The <strong>linear least squares</strong> or <strong>ordinary least squares</strong> (<strong>OLS</strong>) is a linear regression model that takes <span class="math inline">\(f(X) = X\)</span>, the lost function as <span class="math inline">\(\mathbb L_2\)</span> (aka the squared deviance) and the cost function as the mean squared error (aka mean of <span class="math inline">\(\mathbb L_2\)</span> for all rows of <span class="math inline">\(X\)</span>). The linear model is given by,</p>
<p><span class="math display">\[
\hat y(X) = X \theta
\]</span></p>
<p>The optimization problem is then given by,</p>
<p><span class="math display">\[
\begin{align}
\hat \theta &amp;= \mathop{\arg\min}_{\hat \theta} \frac{1}{n} \sum_{i=1}^n \left[y_i - X_i^\top\hat \theta\right]^2\\
&amp;= \mathop{\arg\min}_{\hat \theta} \frac{1}{n} |y-X\hat \theta|^2
\end{align}
\]</span></p>
<p>Solving for best parameters <span class="math inline">\(\hat \theta\)</span>, in general, gives the equation</p>
<p><span class="math display">\[
X^TX \hat \theta = X^T y
\]</span></p>
<p>Therefore we see two types of solution dependent on <span class="math inline">\(X\)</span>:</p>
<ul>
<li><span class="math inline">\(X^\top X\)</span> is invertible with a simple solution.</li>
<li><span class="math inline">\(X^\top X\)</span> is not invertible that are not all solvable.</li>
</ul>
<h3 id="invertible-solutions">Invertible Solutions</h3>
<p>Given the square of the feature matrix <span class="math inline">\(X^\top X\)</span> is invertible then the solution is what’s known as the <strong>normal equation</strong>,</p>
<p><span class="math display">\[
\hat \theta = (X^\top X)^{-1}X^\top y
\]</span></p>
<div class="Note">
<p><span class="math inline">\((X^\top X)^{-1}X^\top\)</span> : This is called the <strong>pseudoinverse</strong> of <span class="math inline">\(X\)</span> often notated as <span class="math inline">\(X^+\)</span>. Notice that,</p>
<p><span class="math display">\[ X^+X = I \]</span></p>
<p>Hence <span class="math inline">\(X^+\)</span> is also known as the <strong>left inverse</strong></p>
<p>Additionally also observe that,</p>
<p><span class="math display">\[ \hat y = X\hat \theta = XX^+ y \]</span></p>
<p>Where <span class="math inline">\(XX^+\)</span> is called the <strong>hat matrix</strong> often notated as <span class="math inline">\(H\)</span>.</p>
</div>
<h3 id="solution-by-projection---normal-equation">Solution by Projection - Normal Equation</h3>
<p>Why is it called the normal equation? Answering that leads to an alternative way to solve for <span class="math inline">\(\hat \theta\)</span> without the need to optimize by calculus. Recall the objective was to essentially minimize <span class="math inline">\(|y-X\hat \theta|^2\)</span> (drop the <span class="math inline">\(1/n\)</span> since it doesn’t matter). We define the residual vector to be <span class="math inline">\(\epsilon = y - X\hat \theta = y - \hat y\)</span>. Because <span class="math inline">\(\hat y\)</span> is the linear combination of the feature columns, it’s span is the feature space while <span class="math inline">\(y\)</span> is does not have to be. Therefore the <span class="math inline">\(\epsilon\)</span> must be a vector orthogonal to <span class="math inline">\(\hat y\)</span> thus also <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
X^\top\epsilon &amp;= X^\top\left[y-X(X^\top X)^{-1}X^\top y\right] \\
&amp;= X^\top y - X^\top y \\
&amp;= 0
\end{align*}
\]</span></p>
<h2 id="polynomial-least-squares">Polynomial Least Squares</h2>
<p>Let the design matrix be denoted as <span class="math inline">\(\Phi(X)\)</span> that transform add polynomials features to the feature matrix.</p>
<p>For example, we can do a degree <span class="math inline">\(k\)</span> polynomial transformation with no cross terms. It’s easier to write this consider every row of <span class="math inline">\(X\)</span> as <span class="math inline">\(X_i\)</span>.</p>
<p><span class="math display">\[
\Phi(X_i^\top) = \begin{bmatrix}
    X_{i1} &amp; X_{i1}^2 &amp; \ldots &amp; X_{i1}^k &amp; \ldots &amp; X_{id} &amp; X_{id}^2 &amp; \ldots &amp; X_{id}^k
\end{bmatrix}
\]</span></p>
<h2 id="weighted-least-squares">Weighted Least Squares</h2>
<p>Sometimes you may want to weigh your data by some sort of importance or confidence metric (e.g., inverse of the data’s error).</p>
<p>For simplicity we are only going to weigh each row. In this case, the weight can be represented as a diagonal weight matrix <span class="math inline">\(\Omega\)</span> such that <span class="math inline">\(\omega_i\)</span> is the weight the row <span class="math inline">\(X_i\)</span>.</p>
<p>The optimization becomes,</p>
<p><span class="math display">\[
\mathop{\arg\min}_{\theta} \sum_{i=1}^n \omega_i(X_i^\top \theta - y_i)^2 = \mathop{\arg\min}_{\theta} (X\theta - y)^\top \Omega (X\theta - y)
\]</span></p>

      </div>
      <!-- Disqus Comments -->
      <div id="disqus_thread" class="mt-5">
        <div class="w-50 mx-auto">
          <button type="button" class="btn btn-block btn-outline-dark show-comments">
            Show Comments
          </button>
        </div>
      </div>
      <!--
                <script>
                /**
                *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
                *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
                var disqus_config = function () {
                    this.page.url = "";  // Replace PAGE_URL with your page's canonical URL variable
                    this.page.identifier = "/notes/Data_Science/Statistical_Modeling/Regression/Linear_Regression/"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
                };
                (function () { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://ketozhang.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                })();
            </script>
            <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered
                    by Disqus.</a></noscript>
            -->
      
    </div>

    <!-- On This Page -->
    <div class="col-md-3 d-none d-md-block">
      <div class="onthispage sidenav small">
    <div class="sidenav-header">
        <strong>&#9776; On This Page</strong>
    </div>
    <div class="onthispage-collapse">
        <div class="sidenav-body bg-light">
            <ul class="onthispage-list no-list-style"></ul>
        </div>
    </div>
</div>
    </div>
  </div>
</div>

    <footer class="py-2 bg-white">
    <div class="container">
        <p class="text-right m-0"> Generated by <a href="https://github.com/ketozhang/StaticPy">StaticPy</a> |
            Designed
            by <a href="https://github.com/ketozhang">Keto Z.</a>
        </p>
    </div>
</footer>

    <!-- Scripts -->
    <script src="https://code.jquery.com/jquery-3.4.1.min.js"
    integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
    crossorigin="anonymous"></script>

    <script src="https://ketozhang.github.io/static/js/datetime.js"></script>
    <script src="https://ketozhang.github.io/static/js/disqus.js"></script>
    <script src="https://ketozhang.github.io/static/js/codeHighlight.js"></script>
    <script src="https://ketozhang.github.io/static/js/prism.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.js"></script>
    <script src="https://ketozhang.github.io/static/js/onthispage.js"></script>
    <script src="https://ketozhang.github.io/static/js/markdown.js"></script>

    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://ketozhang.github.io/static/js/mathjax_config.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>

    <!-- Bootstrap JS -->
    <!-- TODO: trim JS if not needed -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>


    <!-- Custom Global scripts -->
    <script>
      anchors.add();
      $(document).ready(function () {
        Prism.highlightAll();
      })
    </script>

    <!-- Local scripts -->
    
    
</body>

</html>