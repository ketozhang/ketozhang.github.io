<!doctype html>
<html lang='en'>

<head>
    <meta charset="UTF-8">
    <title>StaticPy</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap time -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link href="https://ketozhang.github.io/StaticPy/static/base.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Code+Pro|Quicksand" rel="stylesheet">
    <link href="https://ketozhang.github.io/StaticPy/static/prism.css" rel="stylesheet">
    <link rel="shortcut icon" href="https://ketozhang.github.io/StaticPy/static/favicon.ico">

    <!-- LaTeX Stylesheet -->
    
    <div class="d-none">
    $$
        \newcommand{abs}[1]{\left | #1 \right |}
        \newcommand{set}[1]{\left\{#1\right\}}
    $$
    </div>
    
</head>

<body>
    
<div class="container-fluid" id="note">
    <h1 class="title text-center pt-3">Introduction</h1>
    <hr>
    <div class="row">
        <div class="col-lg-2 d-none d-lg-block"></div>
        <div class="col-lg-7 col-md-10">
            <p>The decision tree is a nonlinear method for classification and regression. The decision tree is a tree with two node types:</p>
<ul>
<li>Internal nodes for each features</li>
<li>Leaf nodes for each class</li>
</ul>
<h2 id="tree-algorithm">Tree Algorithm</h2>
<p>The tree algorithm goes as follows:</p>
<ol style="list-style-type: decimal">
<li>Import all sample points at the top level calling the set of indices <span class="math inline">\(S\)</span></li>
<li>Case:
<ul>
<li>If all sample points belong to a certain class <span class="math inline">\(y_i = c; \exists (c \in C)\forall (i \in S)\)</span> then return a new leaf with class <span class="math inline">\(c\)</span>. This is condition is called when the leaves are <strong>pure</strong>.</li>
<li>Otherwise choose a splitting feature <span class="math inline">\(j\)</span> and splitting boundary <span class="math inline">\(\beta\)</span> where we partition the class into left and right subsets:
<ul>
<li><span class="math inline">\(S_L = \{i : X_{ij} &lt; \beta\}\)</span></li>
<li><span class="math inline">\(S_R = \{i : X_{ij} \ge \beta\}\)</span></li>
</ul></li>
</ul></li>
</ol>
<p>Note that if the first case in step two succeeds then we have a classified all those points where <span class="math inline">\(y_i = c\)</span> as class <span class="math inline">\(c\)</span>.</p>
<h2 id="splitting-feature">Splitting Feature</h2>
<p>Given <span class="math inline">\(d\)</span> splitting features indexed by <span class="math inline">\(j \in \{1,2,\ldots,d\}\)</span>, there are a few way of choosing best splitting feature to implement in the second condition of the tree algorithm at step 2.</p>
<p>The most natural one would be to compute some loss fuction. Let <span class="math inline">\(\mathcal L_j(S)\)</span> be the cost function using splitting feature <span class="math inline">\(j\)</span>. The best split can be chosen to be the one that has the least weighted average cost:</p>
<p><span class="math display">\[
d = \mathop{\arg\min}_{d} \frac{|S_L|\mathcal L_j(S_L) + |S_R|\mathcal L_j(S_R)}{|S_L| + |S_R|}
\]</span></p>
<p>Now the problem left is to choose a loss function. A heuristic choice is the cross entropy.</p>
<h2 id="splitting-boundary">Splitting Boundary</h2>
<p>An algorithm of finding the bondary cost <span class="math inline">\(\mathcal O(n \cdot d \cdot \texttt{depth})\)</span>.</p>
<h2 id="regression">Regression</h2>
<p>Using decision trees to do regression requires choosing piecewise boundaries at every node. At the leaf you may get a subset of sample points which a single value can be outputted by taking some aggregate.</p>
<h2 id="stopping-condition">Stopping Condition</h2>
<p>The tree training algorithm can be stopped to:</p>
<ul>
<li>Limit tree depth</li>
<li>Limit tree size</li>
<li>Prevent overfit</li>
</ul>
<p>The following are examples of stopping conditions * Next split doesn't reduce entropy/error enough * Most of the node points are in the same class * Nodes contains few sample points * Cell's edges are all tiny * Depth is too great * Validation performance does not improve. * Pruning * Remove splits that the removal will improve cross-validation.</p>
<p>When a stopping condition is made the leaves may not be pure thus we need a decision rule for the unpure leaves:</p>
<ul>
<li>Return the majority (classification).</li>
<li>Return the posterior probability.</li>
<li>Return the aggregate (regression).</li>
</ul>
        </div>
        <div class="col-md-2 d-none d-md-block">
            <div class="sticky-top onthispage">
                <ul id="onthispage-list" class="no-list-style">On This Page</ul>
            </div>
        </div>
    </div>
</div>

    <footer class="py-2 bg-white">
    <div class="container">
        <p class="text-right m-0"> Generated by <a href="https://github.com/ketozhang/StaticPy">StaticPy</a> |
            Designed
            by <a href="https://github.com/ketozhang">Keto Z.</a>
        </p>
    </div>
</footer>

    <!-- Scripts -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>

    <script src="https://ketozhang.github.io/StaticPy/static/datetime.js"></script>
    <script src="https://ketozhang.github.io/StaticPy/static/codeHighlight.js"></script>
    <script src="https://ketozhang.github.io/StaticPy/static/prism.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.js"></script>
    <script src="https://ketozhang.github.io/StaticPy/static/onThisPage.js"></script>

    <!-- MathJax -->
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
    </script>
    

    <!-- Bootstrap JS -->
    <!-- TODO: trim JS if not needed -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>

    <!-- React -->
    <!-- TODO: do we need React? -->
    <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/react/15.1.0/react-dom.min.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/react/0.13.3/JSXTransformer.js"></script>-->


    <script>
        anchors.add();
        $(document).ready(function () {
            Prism.highlightAll();
        })
    </script>
</body>

</html>